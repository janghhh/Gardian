import gymnasium as gym
import numpy as np
import airsim
import math
import time
import random


class AirSimMultiDroneEnv:
    metadata = {"render_modes": []}

    def __init__(
        self,
        ip_address="127.0.0.1",
        follower_names=("Follower0", "Follower1", "Follower2"),
        port=41451,
        step_length=0.01,
        leader_velocity=1.0,
        optimal_distance=10.0,
        far_cutoff=60.0,
        too_close=0.5,
        dt=0.1,
        do_visualize=True
    ):
        super().__init__()
        self.possible_agents = list(follower_names)
        self.agents = self.possible_agents[:]

        # ì¶©ëŒ ê´€ë ¨ ì„¤ì •
        self.COLLISION_THRESHOLD = 1.5
        self.STOP_DISTANCE_LEADER_OBSTACLE = 1.0
        
        # ì†ë„/ì•¡ì…˜ ë²„í¼
        self.vmax_self = 2.0
        self._timestep = float(dt)

        # ì—ì´ì „íŠ¸/ë¦¬ë” ì†ë„ ì‚°ì¶œìš© ë²„í¼
        self._last_pose = {}
        self._last_time = {}

        # ì•¡ì…˜ ë²„í¼
        self._last_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self._current_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self.success_steps = {a: 0 for a in self.possible_agents}

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°/í™˜ê²½ íŒŒë¼ë¯¸í„°
        self.step_length = float(step_length)
        self.fixed_z = -10.0
        self.dt = float(dt)
        self.do_visualize = bool(do_visualize)
        self.max_cmd_speed = self.step_length / self.dt
        self.optimal_distance = float(optimal_distance)
        self.far_cutoff = float(far_cutoff)
        self.too_close = float(too_close)
        self.follower_names = list(follower_names)

        # States
        self.step_count = 0
        self.episode_count = 0

        # ë™ì  ì¥ì• ë¬¼ ê´€ë ¨
        self.isIdle = None
        self.D_O_STATE = {0: "idle", 1: "attack"}

        # ===== obs / act / share_obs spaces =====
        self.K_ally = len(follower_names) - 1  # ë‚˜ë¥¼ ì œì™¸í•œ ì•„êµ° ìˆ˜
        self.K_enemy = 1                       # ë™ì  ì¥ì• ë¬¼ 1ëŒ€
        self.num_ally = self.K_ally
        self.num_enemy = self.K_enemy
        
        # ---- ê´€ì¸¡ ê³µê°„ ë²”ìœ„ ì •ì˜ ----
        low_bearing = -1.0
        high_bearing = 1.0
        low_dist = 0.0
        high_dist = 200.0
        
        # closing_speed_norm, los_rate_norm ë²”ìœ„
        low_vel = -1.0
        high_vel = 1.0
        low_rate = -1.0
        high_rate = 1.0

        # [ë¦¬ë”(2)] + [ì•„êµ°(2)*K] + [ì (4)*K]
        per_agent_low = (
            [low_bearing, low_dist] +
            [low_bearing, low_dist] * self.num_ally +
            [low_bearing, low_dist, low_vel, low_rate] * self.num_enemy
        )
        per_agent_high = (
            [high_bearing, high_dist] +
            [high_bearing, high_dist] * self.num_ally +
            [high_bearing, high_dist, high_vel, high_rate] * self.num_enemy
        )

        obs_dim = len(per_agent_low)
        share_obs_dim = obs_dim * len(self.possible_agents)

        self.observation_spaces = {
            agent: gym.spaces.Box(
                low=np.array(per_agent_low, dtype=np.float32),
                high=np.array(per_agent_high, dtype=np.float32),
                shape=(obs_dim,),
                dtype=np.float32,
            )
            for agent in self.possible_agents
        }

        self.MAX_YAW = math.radians(90)
        self.MAX_SPEED = 10

        self.action_spaces = {
            agent: gym.spaces.Box(
                low=-1.0,
                high=1.0,
                shape=(2,),
                dtype=np.float64,
            )
            for agent in self.possible_agents
        }

        self.share_observation_spaces = gym.spaces.Box(
            low=np.array(per_agent_low * len(self.possible_agents), dtype=np.float32),
            high=np.array(per_agent_high * len(self.possible_agents), dtype=np.float32),
            shape=(share_obs_dim,),
            dtype=np.float32,
        )

        self.dynamic_name = "DynamicObstacle"

        # ===== ë³´ìƒ ê´€ë ¨ ë²„í¼/íŒŒë¼ë¯¸í„° =====
        # ì—ì´ì „íŠ¸-ì  ê±°ë¦¬ì˜ ì´ì „ ê°’ ì €ì¥ (ì ‘ê·¼ ë³´ìƒìš©)
        self._prev_d_agent_enemy = {a: None for a in self.possible_agents}

        # íŒ€ ê³µí†µ í„°ë¯¸ë„ ë³´ìƒ
        self.REWARD_HIT_ENEMY = 120.0
        self.REWARD_LEADER_HIT = -300.0
        self.REWARD_AGENT_CRASH = -80.0

        # ì ‘ê·¼ ë³´ìƒ: ì´ì „ë³´ë‹¤ ì–¼ë§ˆë‚˜ ê°€ê¹Œì›Œì¡ŒëŠ”ì§€
        self.W_DIST = 1.0
        self.MAX_DIST_DELTA = 5.0  # í•œ ìŠ¤í…ë‹¹ ìµœëŒ€ ì˜ë¯¸ìˆê²Œ ë³´ëŠ” ì ‘ê·¼ëŸ‰ [m]

        # ë¦¬ë”-ì  ê²½ìŸ(ê°€ë“œ) ë³´ìƒ
        self.W_GUARD = 0.5
        self.ALERT_GUARD_DIST = 80.0  # ì´ ì•ˆì— ë“¤ì–´ì˜¤ë©´ ë°©ì–´ ê´€ì‹¬ êµ¬ê°„

        # ìœ„ì¹˜ ìºì‹œ
        self.start_location = {}
        self.current_location = {}

        # í´ë¼ì´ì–¸íŠ¸ ì…‹ì—…
        self.client = airsim.MultirotorClient(ip=ip_address, port=port)
        self.client.confirmConnection()
        
        self._last_visualize_t = time.time()

    # ======================================================================
    # í—¬í¼ ë©”ì„œë“œ: í¬ì¦ˆ/ì†ë„/ê´€ì¸¡ ê´€ë ¨
    # ======================================================================
    def _angle_and_distance(self, src_drone, target_drone):
        dx = float(
            self.current_location[target_drone].position.x_val -
            self.current_location[src_drone].position.x_val
        )
        dy = float(
            self.current_location[target_drone].position.y_val -
            self.current_location[src_drone].position.y_val
        )
        
        # heading_stateì—ì„œ yaw ì‚¬ìš©
        src_yaw = self.heading_state[src_drone]

        distance_diff = math.sqrt(dx**2 + dy**2)

        _angle = math.atan2(dy, dx)

        angle_diff = ((_angle - src_yaw) + math.pi) % (2 * math.pi) - math.pi
    
        return angle_diff, distance_diff

    def _get_current_location(self):
        self.current_location = {}
        self.current_location["Drone1"] = self.client.simGetObjectPose("Drone1")
        for agent in self.agents:
            self.current_location[agent] = self.client.simGetObjectPose(agent)
        self.current_location[self.dynamic_name] = self.client.simGetObjectPose(self.dynamic_name)

    # ======================================================================
    # ì´ˆê¸°í™”/ì´ë™/ì‹œê°í™” ê´€ë ¨
    # ======================================================================
    def _hover(self, name):
        self.client.moveByVelocityZAsync(
            vx=0.0, vy=0.0,
            z=self.fixed_z,
            duration=0.3,
            vehicle_name=name
        ).join()

        try:
            self.client.hoverAsync(vehicle_name=name).join()
        except:
            pass

    def _setup_flight(self):
        # API Control & Arm
        self.client.enableApiControl(True, vehicle_name="Drone1")
        self.client.armDisarm(True, vehicle_name="Drone1")

        for agent in self.possible_agents:
            self.client.enableApiControl(True, vehicle_name=agent)
            self.client.armDisarm(True, vehicle_name=agent)

        self.client.enableApiControl(True, vehicle_name=self.dynamic_name)
        self.client.armDisarm(True, vehicle_name=self.dynamic_name)

        # Takeoff
        cmds = []
        cmds.append(self.client.takeoffAsync(vehicle_name="Drone1"))
        for agent in self.possible_agents:
            cmds.append(self.client.takeoffAsync(vehicle_name=agent))
        cmds.append(self.client.takeoffAsync(vehicle_name=self.dynamic_name))

        for c in cmds:
            c.join()

        # ì´ˆê¸° pose ê¸°ë¡
        self.start_location["Drone1"] = self.client.simGetObjectPose("Drone1")
        for agent in self.agents:
            self.start_location[agent] = self.client.simGetObjectPose(agent)
        self.start_location[self.dynamic_name] = self.client.simGetObjectPose(self.dynamic_name)

        # ì´ˆê¸° ìœ„ì¹˜ë¡œ ì´ë™ (ë¦¬ë”, ì—ì´ì „íŠ¸, ì  ëª¨ë‘ ì›ì  ì£¼ë³€)
        cmds = []

        cmds.append(
            self.client.moveToPositionAsync(
                x=0.0, y=0.0, z=self.fixed_z,
                velocity=5.0,
                vehicle_name="Drone1"
            )
        )

        for agent in self.possible_agents:
            cmds.append(
                self.client.moveToPositionAsync(
                    x=0.0, y=0.0, z=self.fixed_z,
                    velocity=5.0,
                    vehicle_name=agent
                )
            )

        cmds.append(
            self.client.moveToPositionAsync(
                x=0.0, y=0.0, z=self.fixed_z,
                velocity=5.0,
                vehicle_name=self.dynamic_name
            )
        )

        for c in cmds:
            c.join()

        # Hover ì•ˆì •í™”
        self._hover("Drone1")
        for agent in self.possible_agents:
            self._hover(agent)
        self._hover(self.dynamic_name)

        time.sleep(1)

    def _update_leader_movement(self):
        # ë¦¬ë”ëŠ” ê³ ì •, ì‹œê°í™”ë§Œ
        if self.do_visualize:
            now = time.time()
            if (now - self._last_visualize_t) >= 0.1:
                self.client.simFlushPersistentMarkers()
                self._visualize_circles()
                self._last_visualize_t = now

    def _visualize_circles(self):
        try:
            leader_pos = self.client.simGetObjectPose("Drone1").position
            center = np.array([leader_pos.x_val, leader_pos.y_val, leader_pos.z_val], dtype=float)

            def ring_points(radius, n=36):
                pts = []
                for i in range(n + 1):
                    ang = (i / n) * 2 * np.pi
                    x = center[0] + radius * np.cos(ang)
                    y = center[1] + radius * np.sin(ang)
                    z = center[2]
                    pts.append(airsim.Vector3r(x, y, z))
                return pts

            line_thickness = 20.0
            self.client.simPlotLineStrip(
                ring_points(self.optimal_distance),
                [1, 1, 0, 0.8],
                line_thickness, 0.15, True
            )
            self.client.simPlotLineStrip(
                ring_points(self.far_cutoff),
                [0, 1, 0, 0.8],
                line_thickness, 0.15, True
            )
        except Exception:
            print("ì‹œê°í™” ì˜¤ë¥˜ ë°œìƒ")
            pass

    # ======================================================================
    # ë³´ìƒ/ì¢…ë£Œ ê´€ë ¨
    # ======================================================================
    def _compute_reward(self, agent):
        """
        [ìµœì¢… ë²„ì „]
        - í—¤ë”© ì •ë ¬ ë³´ìƒ ì œê±°
        - ë¡¤(interceptor/defender) ì œê±°
        - ë‘ ê°€ì§€ shapingë§Œ ì‚¬ìš©:
          1) r_approach: ì´ì „ ìŠ¤í…ë³´ë‹¤ ì ê³¼ì˜ ê±°ë¦¬ê°€ ì¤„ì–´ë“¤ë©´ + (ì ‘ê·¼ ë³´ìƒ)
          2) r_guard: ì ì— ëŒ€í•´ ì—ì´ì „íŠ¸ê°€ ë¦¬ë”ë³´ë‹¤ ë” ê°€ê¹ê²Œ ìœ„ì¹˜í•˜ë©´ +, ë’¤ì— ì„œ ìˆìœ¼ë©´ ì•½í•œ -
        """

        eps = 1e-6

        # ë¦¬ë” / ì  ìœ„ì¹˜ (xyë§Œ ì‚¬ìš©)
        enemy_pose = self.current_location[self.dynamic_name].position
        enemy_xy = np.array([enemy_pose.x_val, enemy_pose.y_val], dtype=np.float32)

        leader_pose = self.current_location["Drone1"].position
        leader_xy = np.array([leader_pose.x_val, leader_pose.y_val], dtype=np.float32)

        # ë‚´ ìƒíƒœ
        my_state = self.client.getMultirotorState(vehicle_name=agent)
        my_pos = np.array([
            my_state.kinematics_estimated.position.x_val,
            my_state.kinematics_estimated.position.y_val,
        ], dtype=np.float32)

        # ì—ì´ì „íŠ¸-ì  ê±°ë¦¬
        d_agent_enemy = float(np.linalg.norm(my_pos - enemy_xy))

        # -----------------------------
        # 1) ì ‘ê·¼ ë³´ìƒ (distance improvement)
        # -----------------------------
        r_approach = 0.0
        prev_d = self._prev_d_agent_enemy.get(agent, None)

        if prev_d is not None:
            delta = prev_d - d_agent_enemy   # +ë©´ ì´ì „ë³´ë‹¤ ë” ê°€ê¹Œì›€
            if delta > 0.0:
                delta_clipped = float(
                    np.clip(delta, 0.0, self.MAX_DIST_DELTA)
                )
                # [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”ëœ ì ‘ê·¼ëŸ‰
                r_approach = self.W_DIST * (delta_clipped / self.MAX_DIST_DELTA)

        # í˜„ì¬ ê±°ë¦¬ë¥¼ ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•œ prev ê°’ìœ¼ë¡œ ì €ì¥
        self._prev_d_agent_enemy[agent] = d_agent_enemy

        # -----------------------------
        # 2) ë¦¬ë”-ì  ê²½ìŸ(ê°€ë“œ) ë³´ìƒ
        #    - ì ì´ ë¦¬ë” ê·¼ì²˜(ê²½ë³´ ê±°ë¦¬) ì•ˆì— ë“¤ì–´ì™”ì„ ë•Œë§Œ ì˜ë¯¸ ìˆìŒ
        #    - ì—ì´ì „íŠ¸ê°€ ì ì— ë” ê°€ê¹ê²Œ ì„œ ìˆìœ¼ë©´ ë°©ì–´ë²½ ì—­í•  â†’ +
        #    - ì˜¤íˆë ¤ ì—ì´ì „íŠ¸ê°€ ë” ë©€ë©´, ë¦¬ë”ê°€ ë” ë…¸ì¶œ â†’ ì•½í•œ -
        # -----------------------------
        r_guard = 0.0
        d_leader_enemy = float(np.linalg.norm(leader_xy - enemy_xy))

        if d_leader_enemy < self.ALERT_GUARD_DIST:
            if d_agent_enemy < d_leader_enemy:
                # ë°©ì–´ ì˜ í•˜ëŠ” ìƒí™©
                diff = d_leader_enemy - d_agent_enemy
                ratio = diff / max(d_leader_enemy, eps)  # [0,1] ê·¼ì²˜
                r_guard = self.W_GUARD * ratio
            else:
                # ë¦¬ë”ê°€ ë” ì ì— ê°€ê¹Œìš´ ìƒíƒœ â†’ ì‚´ì§ íŒ¨ë„í‹°
                diff = d_agent_enemy - d_leader_enemy
                ratio = diff / max(d_leader_enemy, eps)
                r_guard = -0.2 * self.W_GUARD * ratio  # íŒ¨ë„í‹°ëŠ” ì‚´ì§ë§Œ

        reward = r_approach + r_guard

        return float(reward), False

    def _end_episode(self, reward, status):
        """
        ì—í”¼ì†Œë“œ ì¢…ë£Œ í—¬í¼ (ì¶©ëŒ/ì„±ê³µ/íƒ€ì„ì•„ì›ƒ ë“± ì´ë²¤íŠ¸ ë°œìƒ ì‹œ)
        - ì—¬ê¸°ì„œëŠ” "íŒ€ ë³´ìƒ" ê°œë… ìœ ì§€:
          í•œ ì—ì´ì „íŠ¸ê°€ ì ì„ ìš”ê²©í•˜ë©´ ëª¨ë‘ REWARD_HIT_ENEMY,
          ë¦¬ë” í”¼ê²©/ì•„êµ° ì¶©ëŒë„ íŒ€ ì „ì²´ì— ë™ì¼ ë³´ìƒ.
        """
        _obs_list = []
        _rewards_list = []
        _terminations_list = []
        _infos_list = []

        is_success = 1 if status == "SUCCESS_DISTANCE_AGENT_DYNAMIC" else 0
        is_leader_hit = 1 if status == "FAIL_LEADER_AND_DYNAMIC_OBSTACLE_COLLISION" else 0
        is_ally_collision = 1 if status in (
            "FAIL_DISTANCE_AGENT_AGENT",
            "FAIL_DISTANCE_AGENT_LEADER",
        ) else 0

        for agent in self.possible_agents:
            _obs_list.append(self._get_obs(agent))
            _rewards_list.append(reward)
            _terminations_list.append(True)

            _infos_list.append({
                agent: {
                    "final_status": status,
                    "reward": reward,
                    "episode_success": is_success,
                    "episode_leader_hit": is_leader_hit,
                    "episode_ally_collision": is_ally_collision,
                }
            })
        
        return _obs_list, _rewards_list, _terminations_list, _infos_list

    # --------------------- ë™ì ì¥ì• ë¬¼ FSM ---------------------
    def _update_dynamic_obstacle(self):
        """
        ë™ì  ì¥ì• ë¬¼ FSM (Step Count ê¸°ë°˜)
        """
        self._obs_step_timer += 1

        if self._obstacle_state == "IDLE":
            # ì œìë¦¬ ëŒ€ê¸°
            self.client.moveByVelocityAsync(
                0, 0, 0, duration=0.1, vehicle_name=self.dynamic_name
            )

            if self._obs_step_timer >= self._idle_wait_steps:
                print(f"[DynamicObstacle] {self._obs_step_timer} steps passed. IDLE -> ATTACK!")
                self._obstacle_state = "ATTACK"
                self._obs_step_timer = 0
        
        elif self._obstacle_state == "ATTACK":
            try:
                leader_pos = self.current_location["Drone1"].position
                obs_pos = self.current_location[self.dynamic_name].position
                
                l_vec = np.array([leader_pos.x_val, leader_pos.y_val, leader_pos.z_val])
                o_vec = np.array([obs_pos.x_val, obs_pos.y_val, obs_pos.z_val])
                
                diff = l_vec - o_vec
                dist = np.linalg.norm(diff)
                
                if dist > 0.5:
                    direction = diff / dist
                    speed = 4.0  # ê³µê²© ì†ë„ (m/s)
                    vel = direction * speed
                    
                    self.client.moveByVelocityAsync(
                        vx=float(vel[0]), vy=float(vel[1]), vz=float(vel[2]),
                        duration=0.1,
                        vehicle_name=self.dynamic_name
                    )
            except Exception as e:
                print(f"Attack Logic Error: {e}")

            if self._obs_step_timer > 500:
                print("[DynamicObstacle] Attack Timeout. Forcing Reset.")
                self._reset_obstacle_logic()

    def _teleport_obstacle_randomly(self):
        """ì¥ì• ë¬¼ì„ ë¦¬ë” ê·¼ì²˜ ëœë¤ ìœ„ì¹˜ë¡œ ìˆœê°„ì´ë™ ì‹œí‚´"""
        leader_pos = self.client.simGetObjectPose("Drone1").position
        lx, ly = leader_pos.x_val, leader_pos.y_val
        
        radius = random.uniform(55.0, 60.0)
        angle = random.uniform(0, 2 * math.pi)
        
        tx = lx + radius * math.cos(angle)
        ty = ly + radius * math.sin(angle)
        tz = self.fixed_z

        pose = airsim.Pose(
            airsim.Vector3r(tx, ty, tz),
            airsim.Quaternionr(0, 0, 0, 1)
        )
        self.client.simSetVehiclePose(pose, True, vehicle_name=self.dynamic_name)

        self.client.moveByVelocityAsync(
            0, 0, 0, duration=0.1, vehicle_name=self.dynamic_name
        ).join()

    def _reset_obstacle_logic(self):
        self._teleport_obstacle_randomly()
        
        self._obstacle_state = "IDLE"
        self._obs_step_timer = 0
        self._idle_wait_steps = random.randint(10, 30)
        
        print(f"[DynamicObstacle] Reset to IDLE. Waiting for {self._idle_wait_steps} steps.")
    
    def _check_distance_collision(self, name_a, name_b, threshold):
        pa = self.current_location[name_a].position
        pb = self.current_location[name_b].position

        dx = pa.x_val - pb.x_val
        dy = pa.y_val - pb.y_val

        dist = math.sqrt(dx*dx + dy*dy)
        return dist < threshold, dist

    # ======================================================================
    # RL/PettingZoo API
    # ======================================================================
    @property
    def observation_space(self):
        return [self.observation_spaces[a] for a in self.possible_agents]

    @property
    def action_space(self):
        return [self.action_spaces[a] for a in self.possible_agents]

    @property
    def share_observation_space(self):
        return [self.share_observation_spaces for _ in self.possible_agents]

    def seed(self, seed):
        random.seed(seed)
        np.random.seed(seed)

    def _get_obs(self, agent):
        # 1. ë‚´ ìƒíƒœ
        my_state = self.client.getMultirotorState(vehicle_name=agent)
        my_pos = np.array([
            my_state.kinematics_estimated.position.x_val,
            my_state.kinematics_estimated.position.y_val
        ], dtype=np.float32)
        my_vel = np.array([
            my_state.kinematics_estimated.linear_velocity.x_val,
            my_state.kinematics_estimated.linear_velocity.y_val
        ], dtype=np.float32)

        # 2. ì  ìƒíƒœ
        target_state = self.client.getMultirotorState(vehicle_name=self.dynamic_name)
        target_pos = np.array([
            target_state.kinematics_estimated.position.x_val,
            target_state.kinematics_estimated.position.y_val
        ], dtype=np.float32)
        target_vel = np.array([
            target_state.kinematics_estimated.linear_velocity.x_val,
            target_state.kinematics_estimated.linear_velocity.y_val
        ], dtype=np.float32)

        # 3. Intercept ë¬¼ë¦¬ëŸ‰
        R_vec = target_pos - my_pos
        V_vec = target_vel - my_vel
        
        dist = float(np.linalg.norm(R_vec))
        epsilon = 1e-6

        closing_speed = -float(np.dot(R_vec, V_vec)) / (dist + epsilon)
        cross_prod = float(R_vec[0] * V_vec[1] - R_vec[1] * V_vec[0])
        los_rate = cross_prod / (dist**2 + epsilon)

        closing_speed = float(np.clip(closing_speed, -30.0, 30.0))
        los_rate      = float(np.clip(los_rate,      -10.0, 10.0))

        closing_speed_norm = closing_speed / 30.0   # [-1,1]
        los_rate_norm      = los_rate      / 10.0   # [-1,1]

        # 4. ê´€ì¸¡ ì¡°ë¦½
        _leader_feats  = []
        _ally_feats    = []
        _dynamic_feats = []

        # ë¦¬ë”: (ìƒëŒ€ ë°©ìœ„, ê±°ë¦¬) -> ê°ë„ [-1,1]ë¡œ ì •ê·œí™”
        angle_diff, distance_diff = self._angle_and_distance(agent, "Drone1")
        angle_norm = angle_diff / math.pi
        _leader_feats = [angle_norm, distance_diff]

        # ì•„êµ°: (ìƒëŒ€ ë°©ìœ„, ê±°ë¦¬)
        other_agents = [a for a in self.possible_agents if a != agent]
        for other in other_agents:
            a_diff, d_diff = self._angle_and_distance(agent, other)
            a_norm = a_diff / math.pi
            _ally_feats.append([a_norm, d_diff])

        # ì : (ìƒëŒ€ ë°©ìœ„, ê±°ë¦¬, closing_speed_norm, los_rate_norm)
        dyn_angle, dyn_dist = self._angle_and_distance(agent, self.dynamic_name)
        dyn_angle_norm = dyn_angle / math.pi

        _dynamic_feats = [
            dyn_angle_norm,
            dyn_dist,
            closing_speed_norm,
            los_rate_norm
        ]

        obs = np.concatenate([
            np.array(_leader_feats,  dtype=np.float32).flatten(),
            np.array(_ally_feats,    dtype=np.float32).flatten(),
            np.array(_dynamic_feats, dtype=np.float32).flatten(),
        ]).astype(np.float32)

        return obs

    def _do_action(self, actions):
        """
        heading_state + speed ì œì–´
        actions[i][0] = yaw_rate  [-1,1] -> [-MAX_YAW, MAX_YAW]
        actions[i][1] = speed     [-1,1] -> [0, MAX_SPEED]
        """

        actions = np.clip(actions, -1, 1)
        dt = 0.1

        for i, agent in enumerate(self.possible_agents):
            a = actions[i]

            # yaw ì—…ë°ì´íŠ¸
            yaw_rate = a[0] * self.MAX_YAW
            self.heading_state[agent] += yaw_rate * dt
            self.heading_state[agent] = (self.heading_state[agent] + math.pi) % (2 * math.pi) - math.pi

            # speed ë³€í™˜
            speed = (a[1] + 1) / 2 * self.MAX_SPEED

            yaw = self.heading_state[agent]
            vx = math.cos(yaw) * speed
            vy = math.sin(yaw) * speed

            self.client.moveByVelocityZAsync(
                vx=vx,
                vy=vy,
                z=self.fixed_z,
                duration=dt,
                vehicle_name=agent
            )

    def _get_rewards(self, per_agent_results):
        # ê° ì—ì´ì „íŠ¸ step ë³´ìƒì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return per_agent_results

    def reset(self, seed=None, options=None):
        self.episode_count += 1
        print(f"ì—í”¼ì†Œë“œ: {self.episode_count} | ì†Œë¹„í•œ ìŠ¤í… ìˆ˜: {self.step_count}")

        self.step_count = 0
        self.agents = self.possible_agents[:]

        # heading_state ì´ˆê¸°í™”
        self.heading_state = {a: 0.0 for a in self.possible_agents}

        # ì›”ë“œ ë¦¬ì…‹ ë° ì´ˆê¸° ë¹„í–‰ ì„¸íŒ…
        self.client.reset()
        self._setup_flight()
        self.client.simFlushPersistentMarkers()

        # ë™ì  ì¥ì• ë¬¼ ì´ˆê¸°í™”
        self._reset_obstacle_logic()

        # í˜„ì¬ ìœ„ì¹˜ ê°±ì‹ 
        self._get_current_location()

        # heading_stateë¥¼ "í˜„ì¬ ì  ë°©í–¥"ìœ¼ë¡œ ì„¸íŒ…
        enemy_pos = self.current_location[self.dynamic_name].position
        ex, ey = enemy_pos.x_val, enemy_pos.y_val

        self.heading_state = {}
        for a in self.possible_agents:
            agent_pos = self.current_location[a].position
            ax, ay = agent_pos.x_val, agent_pos.y_val

            dx = ex - ax
            dy = ey - ay

            desired_yaw = math.atan2(dy, dx)
            desired_yaw = (desired_yaw + math.pi) % (2 * math.pi) - math.pi

            self.heading_state[a] = desired_yaw

        # ì´ì „ ê±°ë¦¬ ë²„í¼ ì´ˆê¸°í™”
        self._prev_d_agent_enemy = {a: None for a in self.possible_agents}

        # ê¸°íƒ€ ë²„í¼ ì´ˆê¸°í™”
        self._last_pose.clear()
        self._last_time.clear()
        self._last_action = {
            a: np.zeros(2, dtype=np.float32) for a in self.possible_agents
        }
        self.leader_stop = False

        # ì—í”¼ì†Œë“œ ì‹œì‘ ê´€ì¸¡
        obs_list = [self._get_obs(a) for a in self.agents]

        print("reset.")

        return obs_list

    def step(self, actions):
        self.step_count += 1

        per_agent_obs, per_agent_results, per_agent_infos = [], [], []

        # 1) ì•¡ì…˜ ì ìš©
        self._do_action(actions)

        # 2) ë¦¬ë”/ì¥ì• ë¬¼ ì—…ë°ì´íŠ¸
        self._update_leader_movement()
        self._update_dynamic_obstacle()

        # 3) ìœ„ì¹˜ ê°±ì‹ 
        self._get_current_location()

        # 4) ì¢…ë£Œì¡°ê±´ ì²´í¬ + ë³´ìƒ ê³„ì‚°
        for agent in self.possible_agents:
            other_agents = [a for a in self.possible_agents if a != agent]

            # ë¦¬ë”ì™€ ê±°ë¦¬
            _distance_leader = np.linalg.norm([
                self.current_location[agent].position.x_val - self.current_location["Drone1"].position.x_val,
                self.current_location[agent].position.y_val - self.current_location["Drone1"].position.y_val,
                self.current_location[agent].position.z_val - self.current_location["Drone1"].position.z_val
            ])

            # ë™ì  ì¥ì• ë¬¼ê³¼ì˜ ê±°ë¦¬
            _distance_dynamic = np.linalg.norm([
                self.current_location[agent].position.x_val - self.current_location[self.dynamic_name].position.x_val,
                self.current_location[agent].position.y_val - self.current_location[self.dynamic_name].position.y_val,
                self.current_location[agent].position.z_val - self.current_location[self.dynamic_name].position.z_val
            ])

            # 4-1) ë²”ìœ„ ì´íƒˆ
            # if _distance_leader > self.far_cutoff:
            #     print(
            #         f"[ì´íƒˆ] {agent}ê°€ ë¦¬ë”ì™€ì˜ ê±°ë¦¬({_distance_leader:.2f}m)ë¡œ, "
            #         f"ì´íƒˆ ì„ê³„ê°’({self.far_cutoff}m) ì´ˆê³¼! â†’ ì „ì²´ ì‹¤íŒ¨(ê²½ê³„ ì´íƒˆ)"
            #     )
            #     return self._end_episode(self.REWARD_AGENT_CRASH, "FAIL_AGENT_FAR_CUTOFF")

            # 4-2) ì—ì´ì „íŠ¸-ë¦¬ë” ì¶©ëŒ
            hit, d = self._check_distance_collision(agent, "Drone1", threshold=self.COLLISION_THRESHOLD)
            if hit:
                print(f"âš ï¸ğŸ’”[ê±°ë¦¬-ì¶©ëŒ] {agent} â†” Drone1  (d={d:.2f}) â†’ ì „ì²´ ì‹¤íŒ¨")
                return self._end_episode(self.REWARD_AGENT_CRASH, "FAIL_DISTANCE_AGENT_LEADER")

            # 4-3) ì—ì´ì „íŠ¸-ì—ì´ì „íŠ¸ ì¶©ëŒ
            for other in other_agents:
                hit, d = self._check_distance_collision(agent, other, threshold=self.COLLISION_THRESHOLD)
                if hit:
                    print(f"ğŸ’¥ğŸ¤–[ê±°ë¦¬-ì¶©ëŒ] {agent} â†” {other}  (d={d:.2f}) â†’ ì „ì²´ ì‹¤íŒ¨")
                    return self._end_episode(self.REWARD_AGENT_CRASH, "FAIL_DISTANCE_AGENT_AGENT")

            # 4-4) ë¦¬ë”-ì  ì¶©ëŒ
            collisionInfo = self.client.simGetCollisionInfo("Drone1")
            if collisionInfo.has_collided and collisionInfo.object_name == self.dynamic_name:
                print(f"ğŸ’¥[ì¶©ëŒ] ìœ ì¸ê¸°ê°€ {collisionInfo.object_name}ì™€ ì¶©ëŒë¡œ â†’ ì „ì²´ ì‹¤íŒ¨")
                return self._end_episode(self.REWARD_LEADER_HIT, "FAIL_LEADER_AND_DYNAMIC_OBSTACLE_COLLISION")

            # 4-5) ì—ì´ì „íŠ¸-ì  ì¶©ëŒ (ìš”ê²© ì„±ê³µ)
            hit, d = self._check_distance_collision(agent, self.dynamic_name, threshold=self.COLLISION_THRESHOLD)
            if hit:
                print(f"ğŸ¯ğŸ”¥[ê±°ë¦¬-ì¶©ëŒ] {agent} â†” {self.dynamic_name}  (d={d:.2f}) â†’ ìš”ê²© ì„±ê³µ")
                return self._end_episode(self.REWARD_HIT_ENEMY, "SUCCESS_DISTANCE_AGENT_DYNAMIC")

            # ì¢…ë£Œ ì•ˆ ë˜ì—ˆìœ¼ë©´ ê´€ì¸¡/ë³´ìƒ ê³„ì‚°
            per_agent_obs.append(self._get_obs(agent))

            _reward, _ = self._compute_reward(agent)
            per_agent_results.append(_reward)
            per_agent_infos.append([f"reward: {_reward}"])

        termination_list = [False for _ in self.possible_agents]
        rewards_list = self._get_rewards(per_agent_results)
        obs_list = per_agent_obs
        infos_list = per_agent_infos

        return obs_list, rewards_list, termination_list, infos_list
