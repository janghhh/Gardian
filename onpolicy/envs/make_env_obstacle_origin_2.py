import gymnasium as gym
import numpy as np
import airsim
import math
import time
import random

class AirSimMultiDroneEnv:
    metadata = {"render_modes": []}

    def __init__(
        self,
        ip_address="127.0.0.1",                 # ê¸°ë³¸ê°’
        follower_names=("Follower0", "Follower1", "Follower2"),
        port=41451,
        step_length=0.01,
        leader_velocity=1.0,
        optimal_distance=10.0,
        far_cutoff=30.0,                        # ìœ ì¸ê¸°ì™€ 30m ì´ìƒ ë–¨ì–´ì§€ë©´ íŒ¨ë„í‹° ë°›ëŠ” êµ¬ê°„
        too_close=0.5,
        dt=0.05,
        do_visualize=True,
        max_step=1000
    ):
        super().__init__()
        self.possible_agents = list(follower_names)
        self.agents = self.possible_agents[:]
        # ê¸°ë³¸ ë³€ìˆ˜ ì„¤ì •
        self.max_step = max_step  # max_step ì¶”ê°€
        self.step_count = 0
        self.episode_count = 0

        # ì¶©ëŒ ê´€ë ¨ ì„¤ì •
        self.COLLISION_THRESHOLD = 1.0  # ëª¨ë“  ê±°ë¦¬ ê¸°ë°˜ ì¶©ëŒ íŒë‹¨ ì„ê³„ê°’ (m)
        self.STOP_DISTANCE_LEADER_OBSTACLE = 1.0  # ìœ ì¸ê¸°-ì¥ì• ë¬¼ ì¶©ëŒ ì„ê³„ê°’ (m)

        # ì†ë„/ì•¡ì…˜ ë²„í¼
        self.vmax_self = 2.0
        self._timestep = 0.05

        # ì—ì´ì „íŠ¸/ë¦¬ë” ì†ë„ ì‚°ì¶œìš© ë²„í¼
        self._last_pose = {}
        self._last_time = {}

        # ì•¡ì…˜ ë²„í¼
        self._last_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self._current_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self.success_steps = {a: 0 for a in self.possible_agents}

        # ğŸ”¹ ì—ì´ì „íŠ¸ë³„ "ì´ì „ ìŠ¤í…ì—ì„œì˜ ì êµ°ê¹Œì§€ ê±°ë¦¬" ì €ì¥ìš© ë²„í¼
        self.prev_distance_dynamic = {a: None for a in self.possible_agents}

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°/í™˜ê²½ íŒŒë¼ë¯¸í„°
        self.step_length = float(step_length)
        self.fixed_z = -10.0
        self.dt = float(dt)
        self.do_visualize = bool(do_visualize)
        self.max_cmd_speed = self.step_length / self.dt
        self.leader_velocity = float(leader_velocity)
        self.optimal_distance = float(optimal_distance)
        self.far_cutoff = float(far_cutoff)
        self.too_close = float(too_close)
        self.follower_names = list(follower_names)

        # States
        self.step_count = 0
        self.episode_count = 0

        ## ë™ì  ì¥ì• ë¬¼ ê´€ë ¨
        self.isIdle = None
        self.D_O_STATE = {0: "idle", 1: "attack"}

        # ë™ì  ì¥ì• ë¬¼ FSM ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
        self._obstacle_state = "IDLE"
        self._obs_step_timer = 0
        self._idle_wait_steps = random.randint(10, 30)

        ### ë¦¬ë”-ë™ì  ì¥ì• ë¬¼ ê±°ë¦¬ íŒ¨ë„í‹°ìš© í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.leader_safe_radius = 20.0          # [m] ì•ˆì „ ë°˜ê²½
        self.max_leader_dynamic_penalty = 50.0  # ì•ˆì „ë°˜ê²½ ì•ˆì—ì„œ ìµœëŒ€ ë¶€ê³¼ë˜ëŠ” ì¶”ê°€ íŒ¨ë„í‹°(ìŒìˆ˜)

        # ===== obs / act / share_obs spaces =====
        self.K_ally = len(follower_names) - 1  # ë‚˜ë¥¼ ì œì™¸í•œ ì•„êµ° ìˆ˜
        self.K_enemy = 1                      # ë™ì  ì¥ì• ë¬¼ 1ëŒ€
        self.num_ally = self.K_ally
        self.num_enemy = self.K_enemy

        # type_flag: 0.0 = ë¦¬ë”, +1.0 = ì•„êµ°, -1.0 = ì êµ°
        self.TYPE_LEADER = 0.0
        self.TYPE_ALLY = 1.0
        self.TYPE_ENEMY = -1.0

        low_bearing = -1.0
        high_bearing = 1.0
        low_dist = 0.0
        high_dist = 200.0
        low_type = -1.0
        high_type = 1.0

        # í•œ íƒ€ê²Ÿë‹¹ [bearing, dist, type_flag]
        per_target_low = [low_bearing, low_dist, low_type]
        per_target_high = [high_bearing, high_dist, high_type]

        # íƒ€ê²Ÿì€ [ë¦¬ë” 1ê°œ + ì•„êµ° num_allyê°œ + ì êµ° num_enemyê°œ]
        num_targets = 1 + self.num_ally + self.num_enemy +1
        per_agent_low = per_target_low * num_targets
        per_agent_high = per_target_high * num_targets

        obs_dim = len(per_agent_low)
        share_obs_dim = obs_dim * len(self.possible_agents)

        self.observation_spaces = {
            agent: gym.spaces.Box(
                low=np.array(per_agent_low, dtype=np.float32),
                high=np.array(per_agent_high, dtype=np.float32),
                shape=(obs_dim,),
                dtype=np.float32
            ) for agent in self.possible_agents
        }

        self.MAX_YAW = 180
        self.MAX_PITCH = 13.0
        # Action ê³µê°„ êµ¬ì„± (Action[0]: Yaw Rate (íšŒì „), Action[1]: Pitch Angle (ìƒí•˜ ê¸°ìš¸ê¸°))
        self.action_spaces = {
            agent: gym.spaces.Box(
                low=-1.0,
                high=1.0,
                shape=(2,),
                dtype=np.float64
            ) for agent in self.possible_agents
        }

        self.share_observation_spaces = gym.spaces.Box(
            low=np.array(per_agent_low * len(self.possible_agents), dtype=np.float32),
            high=np.array(per_agent_high * len(self.possible_agents), dtype=np.float32),
            shape=(share_obs_dim,),
            dtype=np.float32
        )

        self.dynamic_name = "DynamicObstacle"

        # Setting jsonì˜ ì´ˆê¸° ì‹œì‘ ìœ„ì¹˜ ë° ì—í”¼ì†Œë“œ ì´ˆê¸°í™” ì‹œ ì´ˆê¸° ìœ„ì¹˜ (World í”„ë ˆì„ ê¸°ì¤€)
        self.start_location = {}
        self.current_location = {}

        # í´ë¼ì´ì–¸íŠ¸ ì…‹ì—…
        self.client = airsim.MultirotorClient(ip=ip_address, port=port)
        self.client.confirmConnection()

        self._last_visualize_t = time.time()

    # ======================================================================
    # í—¬í¼ ë©”ì„œë“œ: í¬ì¦ˆ/ì†ë„/ê´€ì¸¡ ê´€ë ¨
    # ======================================================================
    def _angle_and_distance(self, src_drone, target_drone):
        # ë‘ ì§€ì ì˜ Xì¶•ê³¼ Yì¶•ì˜ ë³€í™”ëŸ‰
        dx = float(
            self.current_location[target_drone].position.x_val -
            self.current_location[src_drone].position.x_val
        )
        dy = float(
            self.current_location[target_drone].position.y_val -
            self.current_location[src_drone].position.y_val
        )

        # í•´ë‹¹ ì—ì´ì „íŠ¸ì˜ ë°©ìœ„ êµ¬í•˜ê¸°
        src_yaw = airsim.utils.to_eularian_angles(
            self.current_location[src_drone].orientation
        )[2]  # Yaw ë¼ë””ì•ˆ

        # ìƒëŒ€ ê±°ë¦¬ êµ¬í•˜ê¸° (í”¼íƒ€ê³ ë¼ìŠ¤, World Frame ê¸°ì¤€)
        distance_diff = math.sqrt(dx ** 2 + dy ** 2)

        # ìƒëŒ€ ë°©ìœ„ êµ¬í•˜ê¸°
        _angle = math.atan2(dy, dx)
        angle_diff = ((_angle - src_yaw) + math.pi) % (2 * math.pi) - math.pi  # (-pi, +pi)
        angle_norm = angle_diff / math.pi  # [-1, 1]

        return angle_norm, distance_diff

    def _angle_dist_and_type(self, src, tgt, type_flag):
        bearing, dist = self._angle_and_distance(src, tgt)
        return [bearing, dist, type_flag]

    def _get_current_location(self):
        self.current_location = {}  # Init
        self.current_location["Drone1"] = self.client.simGetObjectPose("Drone1")
        for agent in self.agents:
            self.current_location[agent] = self.client.simGetObjectPose(agent)
        self.current_location[self.dynamic_name] = self.client.simGetObjectPose(self.dynamic_name)

    # ======================================================================
    # ì´ˆê¸°í™”/ì´ë™/ì‹œê°í™” ê´€ë ¨
    # ======================================================================
    def _setup_flight(self):

        self.client.enableApiControl(True, vehicle_name="Drone1")
        self.client.armDisarm(True, vehicle_name="Drone1")

        for agent in self.possible_agents:
            self.client.enableApiControl(True, vehicle_name=agent)
            self.client.armDisarm(True, vehicle_name=agent)

        self.client.enableApiControl(True, vehicle_name=self.dynamic_name)
        self.client.armDisarm(True, vehicle_name=self.dynamic_name)

        # ì´ë¥™ ëª…ë ¹ ìƒì„±
        _command = []
        _command.append(self.client.takeoffAsync(vehicle_name="Drone1"))
        for agent in self.possible_agents:
            _command.append(self.client.takeoffAsync(vehicle_name=agent))
        _command.append(self.client.takeoffAsync(vehicle_name=self.dynamic_name))

        for c in _command:
            c.join()

        self.start_location["Drone1"] = self.client.simGetObjectPose("Drone1")
        for agent in self.agents:
            self.start_location[agent] = self.client.simGetObjectPose(agent)
        self.start_location[self.dynamic_name] = self.client.simGetObjectPose(self.dynamic_name)

        # (0,0,fixed_z)ë¡œ ë³µê·€
        _command = []   # Init
        _command.append(
            self.client.moveToPositionAsync(
                x=0.0,
                y=0.0,
                z=self.fixed_z,
                velocity=10.0,
                vehicle_name="Drone1"
            )
        )
        for agent in self.possible_agents:
            _command.append(
                self.client.moveToPositionAsync(
                    x=0.0,
                    y=0.0,
                    z=self.fixed_z,
                    velocity=10.0,
                    vehicle_name=agent
                )
            )
        _command.append(
            self.client.moveToPositionAsync(
                x=0.0,
                y=0.0,
                z=self.fixed_z,
                velocity=10.0,
                vehicle_name=self.dynamic_name
            )
        )

        for c in _command:
            c.join()

        time.sleep(2.0)

    def _update_leader_movement(self):
        """
        ìœ ì¸ê¸°ëŠ” í˜„ì¬ ì •ì§€(hover) ìƒíƒœ ìœ ì§€
        """
        if self.do_visualize:
            now = time.time()
            if (now - self._last_visualize_t) >= 0.1:
                self.client.simFlushPersistentMarkers()
                self._visualize_circles()
                self._last_visualize_t = now

    def _visualize_circles(self):
        try:
            leader_pos = self.client.simGetObjectPose("Drone1").position
            center = np.array([leader_pos.x_val, leader_pos.y_val, leader_pos.z_val], dtype=float)

            def ring_points(radius, n=36):
                pts = []
                for i in range(n + 1):
                    ang = (i / n) * 2 * np.pi
                    x = center[0] + radius * np.cos(ang)
                    y = center[1] + radius * np.sin(ang)
                    z = center[2]
                    pts.append(airsim.Vector3r(x, y, z))
                return pts

            line_thickness = 20.0
            self.client.simPlotLineStrip(
                ring_points(self.optimal_distance),
                [1, 1, 0, 0.8],
                line_thickness, 0.15, True
            )
            self.client.simPlotLineStrip(
                ring_points(self.far_cutoff),
                [0, 1, 0, 0.8],
                line_thickness, 0.15, True
            )
        except Exception:
            print("ì‹œê°í™” ì˜¤ë¥˜ ë°œìƒ")
            pass

    # ======================================================================
    # ë³´ìƒ/ì¢…ë£Œ ê´€ë ¨
    # ======================================================================
    def _compute_reward(self, agent, distance_leader, distance_other,
                        distance_dynamic, distance_dynamic_prev,
                        bearing_dynamic, collided_with_obstacle,
                        dist_leader_dynamic,
                        collided_with_agent):
        """
        ì—ì´ì „íŠ¸ë³„ ë³´ìƒ ê³„ì‚°

        1. ìœ ì¸ê¸°ì™€ ê±°ë¦¬ ìœ ì§€ ë³´ìƒ
           - d <= 5m      : -40
           - 5 < d <= 20m : +100
           - d > 20m      : -100 (ì‹¤ì œë¡œëŠ” step()ì—ì„œ far_cutoff ì´ˆê³¼ ì‹œ ì—í”¼ì†Œë“œ ì¢…ë£Œ)

        2. ê°€ë””ì–¸(ìš”ê²©) ì‹œì•¼ ë³´ìƒ
           - ì êµ°ì„ ì •ë©´(ìƒëŒ€ ë°©ìœ„ê° 0)ì— ë‘ë©´ ë³´ìƒ ìµœëŒ€(+20),
             ì¸¡ë©´/í›„ë°©ì¼ìˆ˜ë¡ ì¤„ì–´ë“¦

        3. ì êµ°ê³¼ì˜ ê±°ë¦¬ ë³´ìƒ (ì ˆëŒ€ ê±°ë¦¬)
           - ì êµ°ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ë³´ìƒ(+30ê¹Œì§€)

        3-1. ìŠ¤í…ë§ˆë‹¤ "ì ‘ê·¼í•œ ì •ë„" ë³´ìƒ (delta ê±°ë¦¬ ê¸°ë°˜)
           - ì´ì „ ìŠ¤í…ë³´ë‹¤ ì êµ°ì—ê²Œ ë” ê°€ê¹Œì›Œì§€ë©´ ì¶”ê°€ ë³´ìƒ(+),
             ë©€ì–´ì§€ë©´ ì†ŒëŸ‰ í˜ë„í‹°(-)

        4. ì—ì´ì „íŠ¸ê°€ ì êµ°ê³¼ ì¶©ëŒ ì‹œ +1000 ë³´ë„ˆìŠ¤ (ìš”ê²© ì„±ê³µ)

        5. ìœ ì¸ê¸°-ë™ì  ì¥ì• ë¬¼ ê±°ë¦¬ í˜ë„í‹°
           - ë¦¬ë” ê·¼ì²˜ë¡œ ì ê¸°ê°€ ë“¤ì–´ì˜¬ìˆ˜ë¡ ì¶”ê°€ í˜ë„í‹°

        6. ì—ì´ì „íŠ¸-ì—ì´ì „íŠ¸ ì¶©ëŒ í˜ë„í‹° (ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—†ì´ ë§¤ ìŠ¤í… -40)
        """

        # 1) ìœ ì¸ê¸°ì™€ ê±°ë¦¬ ìœ ì§€ ë³´ìƒ
        d = distance_leader
        if d <= 1.0:
            r_dist = -40.0
        elif d <= 30.0:
            r_dist = 5.0
        else:
            r_dist = -100.0  # ì‹¤ì œë¡œëŠ” far_cutoffì—ì„œ ì—í”¼ ì¢…ë£Œ
        '''
        # 2) ê°€ë””ì–¸ ì‹œì•¼ ë³´ìƒ (ì êµ°ì„ ì •ë©´ì— ë‘˜ìˆ˜ë¡ ë³´ìƒ)   => ì´ê±°ë¥¼ ì—†ì• ë³´ì !
        if bearing_dynamic is not None:
            # bearing_dynamicì€ ì´ë¯¸ [-1, 1] ë²”ìœ„
            norm_abs = min(abs(bearing_dynamic), 1.0)  # 0(ì •ë©´) ~ 1(ë°˜ëŒ€ ë°©í–¥)
            r_bearing = 20.0 * (1.0 - norm_abs)       # ì •ë©´ì¼ ë•Œ +20, ì˜†/ë’¤ë¡œ ê°ˆìˆ˜ë¡ 0
        else:
            r_bearing = 0.0
        '''

        # 3) ì êµ°ê³¼ì˜ ê±°ë¦¬ ë³´ìƒ (ê°€ê¹Œìš¸ìˆ˜ë¡ ë³´ìƒ)
        max_enemy_reward_dist = 50.0
        clamped = min(distance_dynamic, max_enemy_reward_dist)
        r_enemy_dist = 30.0 * (1.0 - clamped / max_enemy_reward_dist)  # 0m: +30, 50m: 0

        # 3-1) ìŠ¤í…ë§ˆë‹¤ "ì ‘ê·¼í•œ ì •ë„" ë³´ìƒ (delta ê±°ë¦¬ ê¸°ë°˜)
        r_enemy_step = 0.0
        if distance_dynamic_prev is not None:
            delta = distance_dynamic_prev - distance_dynamic  # +ë©´ ê°€ê¹Œì›Œì§, -ë©´ ë©€ì–´ì§
            # í­ì£¼ ë°©ì§€ìš© clamp
            delta = max(min(delta, 5.0), -5.0)  # -5m ~ +5m ë²”ìœ„ë§Œ ë°˜ì˜

            if delta > 0:
                # ê°€ê¹Œì›Œì¡Œì„ ë•Œ ê°•í•œ ë³´ìƒ
                r_enemy_step = 10.0 * delta    # 1m ì ‘ê·¼ ì‹œ +10
            else:
                # ë©€ì–´ì¡Œì„ ë•ŒëŠ” ì•½í•œ í˜ë„í‹°
                r_enemy_step = 5.0 * delta     # 1m ë©€ì–´ì§€ë©´ -5

        # 4) ì êµ°ê³¼ ì¶©ëŒ(ìš”ê²©) ì‹œ ë³´ë„ˆìŠ¤
        r_collision_enemy = 100.0 if collided_with_obstacle else 0.0

        # 5) ìœ ì¸ê¸°-ë™ì  ì¥ì• ë¬¼ ê±°ë¦¬ ê¸°ë°˜ í˜ë„í‹°
        if dist_leader_dynamic < self.leader_safe_radius:
            # closeness: 0 (ê²½ê³„) ~ 1 (ì™„ì „ ê²¹ì¹¨)
            closeness = (self.leader_safe_radius - dist_leader_dynamic) / self.leader_safe_radius
            closeness = max(0.0, min(1.0, closeness))
            r_leader_safety = - self.max_leader_dynamic_penalty * closeness
        else:
            r_leader_safety = 0.0

        # 6) ì—ì´ì „íŠ¸-ì—ì´ì „íŠ¸ ì¶©ëŒ í˜ë„í‹° (ì—í”¼ ì¢…ë£Œ ì—†ì´ ë§¤ ìŠ¤í… ë¶€ê³¼) => ì§€ê¸ˆ ì—ì´ì „íŠ¸ì™€ ì—ì´ì „íŠ¸ ì¶©ëŒ íšŒí”¼ ë³´ìƒ ì•„ì˜ˆ ì—†ì•  ë†“ìŒ
        #r_agent_collision = -40.0 if collided_with_agent else 0.0
        # + r_agent_collision
        r_total = (
            r_dist
            + r_enemy_dist
            + r_enemy_step
            + r_collision_enemy
            + r_leader_safety
        )
        return float(r_total)

    def _end_episode(self, reward, status):
        """
        ì—í”¼ì†Œë“œ ì¢…ë£Œ í—¬í¼ (ì¶©ëŒ ì´ë²¤íŠ¸/ì´íƒˆ ë“± ë°œìƒ ì‹œ)
        - í˜„ì¬ êµ¬í˜„ì€ ëª¨ë“  ì—ì´ì „íŠ¸ì—ê²Œ ë™ì¼í•œ ì¢…ë£Œ ë³´ìƒì„ ë¶€ì—¬
        """
        _obs_list = []
        _rewards_list = []
        _terminations_list = []
        _infos_list = []

        for agent in self.possible_agents:
            _obs_list.append(self._get_obs(agent))
            _rewards_list.append(reward)
            _terminations_list.append(True)
            _infos_list.append({agent: {"final_status": status, "reward": reward}})

        return _obs_list, _rewards_list, _terminations_list, _infos_list

    # --------------------- ë™ì ì¥ì• ë¬¼ FSM ---------------------
    def _update_dynamic_obstacle(self):
        """
        ë™ì  ì¥ì• ë¬¼ FSM (Step Count ê¸°ë°˜)
        """
        self._obs_step_timer += 1  # í˜„ì¬ ìƒíƒœì—ì„œì˜ ê²½ê³¼ ìŠ¤í… ì¦ê°€

        if self._obstacle_state == "IDLE":
            self.client.moveByVelocityAsync(0, 0, 0, duration=0.1, vehicle_name=self.dynamic_name)

            if self._obs_step_timer >= self._idle_wait_steps:
                print(f"[DynamicObstacle] {self._obs_step_timer} steps passed. IDLE -> ATTACK!")
                self._obstacle_state = "ATTACK"
                self._obs_step_timer = 0

        elif self._obstacle_state == "ATTACK":
            try:
                leader_pos = self.current_location["Drone1"].position
                obs_pos = self.current_location[self.dynamic_name].position

                l_vec = np.array([leader_pos.x_val, leader_pos.y_val, leader_pos.z_val])
                o_vec = np.array([obs_pos.x_val, obs_pos.y_val, obs_pos.z_val])

                diff = l_vec - o_vec
                dist = np.linalg.norm(diff)

                if dist > 0.5:
                    direction = diff / dist
                    speed = 2.0  # ê³µê²© ì†ë„ (m/s)
                    vel = direction * speed

                    self.client.moveByVelocityAsync(
                        vx=float(vel[0]), vy=float(vel[1]), vz=float(vel[2]),
                        duration=0.1,
                        vehicle_name=self.dynamic_name
                    )
            except Exception as e:
                print(f"Attack Logic Error: {e}")

            if self._obs_step_timer > 1000:
                print("[DynamicObstacle] Attack Timeout. Forcing Reset.")
                self._reset_obstacle_logic()

    def _teleport_obstacle_randomly(self):
        """ì¥ì• ë¬¼ì„ ë¦¬ë” ê·¼ì²˜ ëœë¤ ìœ„ì¹˜ë¡œ ìˆœê°„ì´ë™ ì‹œí‚´"""
        leader_pos = self.client.simGetObjectPose("Drone1").position
        lx, ly = leader_pos.x_val, leader_pos.y_val

        radius = random.uniform(50.0, 60.0)
        angle = random.uniform(0, 2 * math.pi)

        tx = lx + radius * math.cos(angle)
        ty = ly + radius * math.sin(angle)
        tz = self.fixed_z

        pose = airsim.Pose(airsim.Vector3r(tx, ty, tz), airsim.Quaternionr(0, 0, 0, 1))
        self.client.simSetVehiclePose(pose, ignore_collision=True, vehicle_name=self.dynamic_name)

        self.client.moveByVelocityAsync(0, 0, 0, duration=0.1, vehicle_name=self.dynamic_name).join()

    def _reset_obstacle_logic(self):
        self._teleport_obstacle_randomly()

        self._obstacle_state = "IDLE"
        self._obs_step_timer = 0
        self._idle_wait_steps = random.randint(10, 30)

        print(f"[DynamicObstacle] Reset to IDLE. Waiting for {self._idle_wait_steps} steps.")

    # ======================================================================
    # RL/PettingZoo API
    # ======================================================================
    @property
    def observation_space(self):
        return [self.observation_spaces[a] for a in self.possible_agents]

    @property
    def action_space(self):
        return [self.action_spaces[a] for a in self.possible_agents]

    @property
    def share_observation_space(self):
        return [self.share_observation_spaces for _ in self.possible_agents]

    def seed(self, seed):
        random.seed(seed)
        np.random.seed(seed)

    def _get_obs(self, agent):

        leader_feats = self._angle_dist_and_type(agent, "Drone1", self.TYPE_LEADER)

        ally_feats = []
        other_agents = [a for a in self.possible_agents if a != agent]
        for other in other_agents:
            ally_feats.append(self._angle_dist_and_type(agent, other, self.TYPE_ALLY))

        dynamic_feats = []
        dynamic_feats.append(self._angle_dist_and_type(agent, self.dynamic_name, self.TYPE_ENEMY))

        # ë¦¬ë”-ë™ì ì¥ì• ë¬¼ ê±°ë¦¬ ê´€ì¸¡ ì¶”ê°€ (bearing=0, type=-1 ë¡œ ì²˜ë¦¬)
        leader_pos = self.current_location["Drone1"].position
        dyn_pos = self.current_location[self.dynamic_name].position
        dist_leader_dynamic = np.linalg.norm([
            leader_pos.x_val - dyn_pos.x_val,
            leader_pos.y_val - dyn_pos.y_val,
            leader_pos.z_val - dyn_pos.z_val
        ])
        
        leader_dynamic_feat = np.array([[0.0, dist_leader_dynamic, -1.0]], dtype=np.float32)

        feats = np.concatenate([
            np.array(leader_feats, dtype=np.float32).reshape(1, -1),
            np.array(ally_feats, dtype=np.float32),
            np.array(dynamic_feats, dtype=np.float32),
            leader_dynamic_feat
        ], axis=0)

        return feats.flatten().astype(np.float32)



    def _do_action(self, actions):
        actions = np.clip(actions, -1.0, 1.0)
        futures = []
        for i, agent in enumerate(self.possible_agents):
            _yaw = math.radians(actions[i][0] * self.MAX_YAW)
            _pitch = math.radians(actions[i][1] * self.MAX_PITCH)
            f = self.client.moveByRollPitchYawrateZAsync(
                roll=0.0,
                pitch=_pitch,
                yaw_rate=_yaw,
                z=self.fixed_z,
                duration=self._timestep,
                vehicle_name=agent
            )
            futures.append(f)
        for f in futures:
            f.join()

    def _get_rewards(self, per_agent_results):
        return per_agent_results

    def reset(self, seed=None, options=None):

        self.episode_count += 1
        print(f"Current Episode: {self.episode_count}")

        self.agents = self.possible_agents[:]

        self.client.reset()
        self._setup_flight()
        self.client.simFlushPersistentMarkers()

        self._reset_obstacle_logic()

        self._get_current_location()

        self.step_count = 0

        self._last_pose.clear()
        self._last_time.clear()
        self._last_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self.leader_stop = False

        # ğŸ”¹ ì—ì´ì „íŠ¸ë³„ ì´ˆê¸° ì êµ° ê±°ë¦¬ ì €ì¥
        for agent in self.possible_agents:
            self.prev_distance_dynamic[agent] = np.linalg.norm([
                self.current_location[agent].position.x_val - self.current_location[self.dynamic_name].position.x_val,
                self.current_location[agent].position.y_val - self.current_location[self.dynamic_name].position.y_val,
                self.current_location[agent].position.z_val - self.current_location[self.dynamic_name].position.z_val
            ])

        obs_list = [self._get_obs(a) for a in self.agents]

        print("reset.")

        return obs_list

    def step(self, actions):

        self.step_count += 1
        per_agent_obs, per_agent_results, per_agent_infos = [], [], []

        # ===== Action Step =====
        self._do_action(actions)
        self._update_leader_movement()
        self._update_dynamic_obstacle()
        self._get_current_location()

        ### ë¦¬ë”-ë™ì  ì¥ì• ë¬¼ ê±°ë¦¬ ê³„ì‚° (ëª¨ë“  ì—ì´ì „íŠ¸ ë³´ìƒì— ê³µí†µ ì‚¬ìš©)
        leader_pos = self.current_location["Drone1"].position
        dyn_pos = self.current_location[self.dynamic_name].position
        dist_leader_dynamic = np.linalg.norm([
            leader_pos.x_val - dyn_pos.x_val,
            leader_pos.y_val - dyn_pos.y_val,
            leader_pos.z_val - dyn_pos.z_val
        ])

        # ìœ ì¸ê¸°-ë™ì  ì¥ì• ë¬¼ ì¶©ëŒ ì²´í¬ (ì—í”¼ì†Œë“œ ì¢…ë£Œ + í° íŒ¨ë„í‹°)
        leader_collision = self.client.simGetCollisionInfo("Drone1")
        leader_dynamic_collision = False
        if leader_collision.has_collided and leader_collision.object_name == self.dynamic_name:
            print(f"[ì¶©ëŒ] ìœ ì¸ê¸°ê°€ {leader_collision.object_name}ì™€ ì¶©ëŒ")
            #return self._end_episode(-100.0, "FAIL_LEADER_AND_DYNAMIC_OBSTACLE_COLLISION")
            leader_dynamic_collision = True

        # ===== Check Termination & Reward Step =====
        for agent in self.possible_agents:

            other_agents = [a for a in self.possible_agents if a != agent]

            _distance_leader = np.linalg.norm([
                self.current_location[agent].position.x_val - self.current_location["Drone1"].position.x_val,
                self.current_location[agent].position.y_val - self.current_location["Drone1"].position.y_val,
                self.current_location[agent].position.z_val - self.current_location["Drone1"].position.z_val
            ])
            _distance_other = [
                np.linalg.norm([
                    self.current_location[agent].position.x_val - self.current_location[other].position.x_val,
                    self.current_location[agent].position.y_val - self.current_location[other].position.y_val,
                    self.current_location[agent].position.z_val - self.current_location[other].position.z_val
                ]) for other in other_agents
            ]
            _distance_dynamic = np.linalg.norm([
                self.current_location[agent].position.x_val - self.current_location[self.dynamic_name].position.x_val,
                self.current_location[agent].position.y_val - self.current_location[self.dynamic_name].position.y_val,
                self.current_location[agent].position.z_val - self.current_location[self.dynamic_name].position.z_val
            ])
            '''
            # 1) ìœ ì¸ê¸°ì™€ì˜ ê±°ë¦¬ ê¸°ë°˜ ì¢…ë£Œ ì¡°ê±´
            if _distance_leader > self.far_cutoff:
                print(f"[ì´íƒˆ] {agent}ê°€ ë¦¬ë”ì™€ì˜ ê±°ë¦¬({_distance_leader:.2f}m)ë¡œ, ì´íƒˆ ì„ê³„ê°’({self.far_cutoff}m) ì´ˆê³¼! â†’ ì „ì²´ ì‹¤íŒ¨ ë° ì¢…ë£Œ")
                return self._end_episode(-100.0, "FAIL_AGENT_FAR_CUTOFF")
            '''
            collided_with_obstacle = False
            collided_with_agent = False
            collided_with_leader = False
            # 2) ì—ì´ì „íŠ¸/ìœ ì¸ê¸°/ì—ì´ì „íŠ¸ ê°„ ì¶©ëŒ ì²´í¬
            collided_with_obstacle = False
            collided_with_agent = False

            collisionInfo = self.client.simGetCollisionInfo(vehicle_name=agent)
            if collisionInfo.has_collided:
                if collisionInfo.object_name == "Drone1":
                    print(f"[ì¶©ëŒ] {agent}ê°€ ë¦¬ë”ì™€ ì¶©ëŒ")
                    #return self._end_episode(-40.0, "FAIL_AGENT_AND_LEADER_COLLISION")
                elif collisionInfo.object_name in other_agents:
                    # ğŸ”¹ ì—ì´ì „íŠ¸-ì—ì´ì „íŠ¸ ì¶©ëŒ: ì—í”¼ì†Œë“œ ì¢…ë£ŒëŠ” í•˜ì§€ ì•Šê³  ë§¤ ìŠ¤í… íŒ¨ë„í‹°ë§Œ ë¶€ì—¬
                    print(f"[ì¶©ëŒ] {agent}ê°€ {collisionInfo.object_name}ì™€ ì¶©ëŒ")
                    collided_with_agent = True
                elif collisionInfo.object_name == self.dynamic_name:
                    print(f"[ì¶©ëŒ] {agent}ê°€ ë™ì  ì¥ì• ë¬¼ê³¼ ì¶©ëŒ â†’ ë³´ìƒ +100, ì—í”¼ì†Œë“œëŠ” ê³„ì† ì§„í–‰")
                    collided_with_obstacle = True

            per_agent_obs.append(self._get_obs(agent))

            bearing_dynamic, _ = self._angle_and_distance(agent, self.dynamic_name)

            #  ì´ì „ ìŠ¤í…ì—ì„œì˜ ì êµ° ê±°ë¦¬
            _prev_distance_dynamic = self.prev_distance_dynamic.get(agent, None)

            _reward = self._compute_reward(
                agent,
                _distance_leader,
                _distance_other,
                _distance_dynamic,
                _prev_distance_dynamic,      # delta ê¸°ë°˜ ë³´ìƒì— ì‚¬ìš©
                bearing_dynamic,
                collided_with_obstacle,
                dist_leader_dynamic,
                collided_with_agent
            )

            # Add penalties
            if collided_with_leader:
                _reward -= 40.0  # ìœ ì¸ê¸°ì™€ ì¶©ëŒ ì‹œ íŒ¨ë„í‹°
            if leader_dynamic_collision:
                _reward -= 100.0  # ìœ ì¸ê¸°-ì êµ° ì¶©ëŒ ì‹œ íŒ¨ë„í‹°

            # Append results
            per_agent_results.append(_reward)
            per_agent_infos.append(f"reward: {_reward}")  # ìˆ˜ì •ëœ êµ¬ë¬¸

             #  ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´ í˜„ì¬ ì êµ° ê±°ë¦¬ ì €ì¥
            self.prev_distance_dynamic[agent] = _distance_dynamic

        # max_stepì— ë„ë‹¬í–ˆëŠ”ì§€ ì²´í¬
        termination_list = [False for _ in self.possible_agents]
        if self.step_count >= self.max_step:
            print(f"Max step {self.max_step} reached. Ending episode.")
            termination_list = [True for _ in self.possible_agents]  # ëª¨ë“  ì—ì´ì „íŠ¸ë¥¼ ì¢…ë£Œ ìƒíƒœë¡œ ì„¤ì •

        rewards_list = self._get_rewards(per_agent_results)
        obs_list = per_agent_obs
        infos_list = per_agent_infos

        return obs_list, rewards_list, termination_list, infos_list

