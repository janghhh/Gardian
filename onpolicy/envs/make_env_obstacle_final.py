import gymnasium as gym
import numpy as np
import airsim
import math
import time
import random



class AirSimMultiDroneEnv:
    metadata = {"render_modes": []}


    """double RNN ì ìš© í•´ë´? ì êµ°ì˜ ê²½ë¡œ ì•ˆì— ë“¤ì–´ì™€ì„œ í”¼ê²©í•˜ê² ë” ì–´ë–»ê²Œ ìœ ë„í•˜ì§€? ì–´ë–»ê²Œ ì˜ˆì¸¡ì„ í•˜ê² ë” ë³´ìƒì—ì„œ ìœ ë„í•  ìˆ˜ ìˆì„ê¹Œ, ì™œ ë¹—ê²¨ë‚˜ê°ˆê¹Œ?"""

    def __init__(
        self,
        ip_address="127.0.0.1",
        follower_names=("Follower0", "Follower1", "Follower2"),
        port=41451,
        step_length=0.01,
        leader_velocity=1.0,
        optimal_distance=10.0,
        far_cutoff=60.0,
        too_close=0.5,
        dt=0.01,
        do_visualize=True
    ):
        super().__init__()
        self.possible_agents = list(follower_names)
        self.agents = self.possible_agents[:]

        # ì¶©ëŒ ê´€ë ¨ ì„¤ì •
        self.COLLISION_THRESHOLD = 2.0
        self.STOP_DISTANCE_LEADER_OBSTACLE = 1.0
        
        # ì†ë„/ì•¡ì…˜ ë²„í¼
        self.vmax_self = 2.0
        self._timestep = float(dt)   # dtì™€ ë§ì¶°ì£¼ê±°ë‚˜, ì“°ì§€ ì•Šìœ¼ë©´ ì œê±°í•´ë„ OK

        # ì—ì´ì „íŠ¸/ë¦¬ë” ì†ë„ ì‚°ì¶œìš© ë²„í¼
        self._last_pose = {}
        self._last_time = {}

        # ì•¡ì…˜ ë²„í¼
        self._last_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self._current_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self.success_steps = {a: 0 for a in self.possible_agents}

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°/í™˜ê²½ íŒŒë¼ë¯¸í„°
        self.step_length = float(step_length)
        self.fixed_z = -10.0
        self.dt = float(dt)
        self.do_visualize = bool(do_visualize)
        self.max_cmd_speed = self.step_length / self.dt
        self.leader_velocity = float(leader_velocity)
        self.optimal_distance = float(optimal_distance)
        self.far_cutoff = float(far_cutoff)
        self.too_close = float(too_close)
        self.follower_names = list(follower_names)

        # States
        self.step_count = 0
        self.episode_count = 0

        # ë™ì  ì¥ì• ë¬¼ ê´€ë ¨
        self.isIdle = None
        self.D_O_STATE = {0: "idle", 1: "attack"}
 
        # ===== obs / act / share_obs spaces =====
        self.K_ally = len(follower_names) - 1  # ë‚˜ë¥¼ ì œì™¸í•œ ì•„êµ° ìˆ˜
        self.K_enemy = 1                       # ë™ì  ì¥ì• ë¬¼ 1ëŒ€
        self.num_ally = self.K_ally
        self.num_enemy = self.K_enemy
        
        # ---- ê´€ì¸¡ ê³µê°„ ë²”ìœ„ ì •ì˜ ----
        low_bearing = -1.0
        high_bearing = 1.0
        low_dist = 0.0
        high_dist = 200.0
        
        # â˜… ì‹¤ì œ ë„£ëŠ” ê°’ì€ [-1,1] ì´ë¯€ë¡œ ì—¬ê¸°ë„ ë§ì¶°ì¤Œ
        low_vel = -1.0      # ì •ê·œí™”ëœ closing_speed_norm
        high_vel = 1.0
        low_rate = -1.0     # ì •ê·œí™”ëœ los_rate_norm
        high_rate = 1.0

        # [ë¦¬ë”(2)] + [ì•„êµ°(2)*K] + [ì (4)*K]
        per_agent_low = (
            [low_bearing, low_dist] +
            [low_bearing, low_dist] * self.num_ally +
            [low_bearing, low_dist, low_vel, low_rate] * self.num_enemy
        )
        per_agent_high = (
            [high_bearing, high_dist] +
            [high_bearing, high_dist] * self.num_ally +
            [high_bearing, high_dist, high_vel, high_rate] * self.num_enemy
        )

        obs_dim = len(per_agent_low)
        share_obs_dim = obs_dim * len(self.possible_agents)

        self.observation_spaces = {
            agent: gym.spaces.Box(
                low=np.array(per_agent_low, dtype=np.float32),
                high=np.array(per_agent_high, dtype=np.float32),
                shape=(obs_dim,),
                dtype=np.float32,
            )
            for agent in self.possible_agents
        }

        self.MAX_YAW = math.radians(90)
        self.MAX_SPEED = 5

        self.action_spaces = {
            agent: gym.spaces.Box(
                low=-1.0,
                high=1.0,
                shape=(2,),
                dtype=np.float64,
            )
            for agent in self.possible_agents
        }

        self.share_observation_spaces = gym.spaces.Box(
            low=np.array(per_agent_low * len(self.possible_agents), dtype=np.float32),
            high=np.array(per_agent_high * len(self.possible_agents), dtype=np.float32),
            shape=(share_obs_dim,),
            dtype=np.float32,
        )

        self.dynamic_name = "DynamicObstacle"
        # PN ë³´ìƒìš© ë²„í¼ë“¤
        self._prev_d_leader_enemy = None
        self._prev_d_agent_enemy = {a: None for a in self.possible_agents}
        self._prev_los_angle = {a: None for a in self.possible_agents}

        # PN-style reward parameters
        self.REWARD_HIT_ENEMY = 500.0
        self.REWARD_LEADER_HIT = -300.0
        self.REWARD_AGENT_CRASH = -80.0

        self.W_CLOSE = 1.0
        self.W_LOS = 1.0
        self.W_DIST = 1.0

        self.MAX_DIST_DELTA = 5.0
        self.MAX_ANGLE_DELTA = math.radians(20.0)    
                     
        self.W_DEF_LEADER   = 0.2   # ìˆ˜ë¹„ ë³´ìƒ ìµœëŒ€ê°’ (ë„ˆë¬´ í¬ì§€ ì•Šê²Œ)
        self.DEF_LEADER_BAND = 5.0  # [optimal_distance Â± 5m] ì•ˆì´ë©´ ë³´ìƒ

        # ìœ„ì¹˜ ìºì‹œ
        self.start_location = {}
        self.current_location = {}

        # í´ë¼ì´ì–¸íŠ¸ ì…‹ì—…
        self.client = airsim.MultirotorClient(ip=ip_address, port=port)
        self.client.confirmConnection()
        
        self._last_visualize_t = time.time()


    # ======================================================================
    # í—¬í¼ ë©”ì„œë“œ: í¬ì¦ˆ/ì†ë„/ê´€ì¸¡ ê´€ë ¨
    # ======================================================================
    def _angle_and_distance(self, src_drone, target_drone):

        # ë‘ ì§€ì ì˜ Xì¶•ê³¼ Yì¶•ì˜ ë³€í™”ëŸ‰
        dx = float(
            self.current_location[target_drone].position.x_val - 
            self.current_location[src_drone].position.x_val
        )
        dy = float(
            self.current_location[target_drone].position.y_val - 
            self.current_location[src_drone].position.y_val        
        )
        
        # ë‚´ê°€ ì“°ëŠ” heading_state ì°¸ì¡°
        src_yaw = self.heading_state[src_drone]

        # ìƒëŒ€ ê±°ë¦¬ êµ¬í•˜ê¸° (í”¼íƒ€ê³ ë¼ìŠ¤, World Frame ê¸°ì¤€)
        distance_diff = math.sqrt(dx**2 + dy**2)

        # ìƒëŒ€ ë°©ìœ„ êµ¬í•˜ê¸°
        ## - World Frame ê¸°ì¤€ ë‘ ì¢Œí‘œì˜ ìƒëŒ€ ë°©ìœ„ êµ¬í•˜ê¸° (arctan í™œìš©)
        _angle = math.atan2(dy, dx) # ì¶•ì´ 90Â° í‹€ì–´ì ¸ ìˆìŒ dx, dy -> dy, dxë¡œ ìˆ˜ì •

        ## - ë‘ ë°©ìœ„ë¥¼ ë¹¼ë©´ ë“œë¡  ê¸°ì¤€ ìƒëŒ€ ë°©ìœ„ë¥¼ í•  ìˆ˜ ìˆë‹¤. (ì ˆëŒ€ ë°©ìœ„ ì°¨ - í˜„ì¬ ë“œë¡ ì˜ ë°©ìœ„)
        angle_diff = ((_angle - src_yaw) + math.pi) % (2 * math.pi) - math.pi   # ê°ë„ ì •ê·œí™” (-180, +180)ìœ¼ë¡œ ì •ê·œí™”
    
        return angle_diff, distance_diff

    def _get_current_location(self):
        self.current_location = {}  # Init
        self.current_location["Drone1"] = self.client.simGetObjectPose("Drone1")
        for agent in self.agents:
            self.current_location[agent] = self.client.simGetObjectPose(agent)
        self.current_location[self.dynamic_name] = self.client.simGetObjectPose(self.dynamic_name)

    # ======================================================================
    # ì´ˆê¸°í™”/ì´ë™/ì‹œê°í™” ê´€ë ¨
    # ======================================================================
    def _hover(self, name):
        """ë“œë¡ ì„ ì™„ì „ ì •ì§€ì‹œí‚¤ëŠ” í˜¸ë²„ë§ í•¨ìˆ˜"""
        # velocity 0ìœ¼ë¡œ ì£¼ê³  PID ì•ˆì •í™”
        self.client.moveByVelocityZAsync(
            vx=0.0, vy=0.0,
            z=self.fixed_z,
            duration=0.3,
            vehicle_name=name
        ).join()

        # AirSim ë‚´ë¶€ hover PID í™œì„±í™”
        try:
            self.client.hoverAsync(vehicle_name=name).join()
        except:
            pass  # ì¼ë¶€ AirSim ë²„ì „ì—ì„œ hoverAsync ì—†ì„ ìˆ˜ ìˆìŒ

    def _setup_flight(self):
        # 1. API ì œì–´ê¶Œ ë° ì‹œë™ (Arming)
        self.client.enableApiControl(True, vehicle_name="Drone1")
        self.client.armDisarm(True, vehicle_name="Drone1")
        for agent in self.possible_agents:
            self.client.enableApiControl(True, vehicle_name=agent)
            self.client.armDisarm(True, vehicle_name=agent)
        self.client.enableApiControl(True, vehicle_name=self.dynamic_name)
        self.client.armDisarm(True, vehicle_name=self.dynamic_name)

        # 2. ì´ë¥™ (Takeoff) - ìœ„ì¹˜ ì´ë™ì€ í•˜ì§€ ì•ŠìŒ
        cmds = []
        cmds.append(self.client.takeoffAsync(vehicle_name="Drone1"))
        for agent in self.possible_agents:
            cmds.append(self.client.takeoffAsync(vehicle_name=agent))
        cmds.append(self.client.takeoffAsync(vehicle_name=self.dynamic_name))

        for c in cmds:
            c.join()
        
        # ìœ„ì¹˜ ì´ë™ ë¡œì§ ì‚­ì œë¨ -> resetì—ì„œ ì²˜ë¦¬


    def _update_leader_movement(self):
        """
        ìœ ì¸ê¸°ì— ì•„ë¬´ëŸ° ëª…ë ¹ë„ ë‚´ë¦¬ì§€ ì•Šê³ , ê·¸ëŒ€ë¡œ ìœ ì§€, ìœ ì¸ê¸°ëŠ” ì´ˆê¸° setupì‹œì— í•´ë‹¹ ê³ ë„ë¡œ ì´ë™í•œ í›„, ì•„ë¬´ëŸ° ëª…ë ¹ì—†ì´ ëŒ€ê¸°
        """

        # 2. ì‹œê°í™”ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        if self.do_visualize:
            now = time.time()
            if (now - self._last_visualize_t) >= 0.1:
                self.client.simFlushPersistentMarkers()
                self._visualize_circles()
                self._last_visualize_t = now

    def _visualize_circles(self):
        try:
            leader_pos = self.client.simGetObjectPose("Drone1").position
            center = np.array([leader_pos.x_val, leader_pos.y_val, leader_pos.z_val], dtype=float)

            def ring_points(radius, n=36):
                pts = []
                for i in range(n + 1):
                    ang = (i / n) * 2 * np.pi
                    x = center[0] + radius * np.cos(ang)
                    y = center[1] + radius * np.sin(ang)
                    z = center[2]
                    pts.append(airsim.Vector3r(x, y, z))
                return pts

            line_thickness = 20.0
            self.client.simPlotLineStrip(
                ring_points(self.optimal_distance),
                [1, 1, 0, 0.8],
                line_thickness, 0.15, True
            )
            self.client.simPlotLineStrip(
                ring_points(self.far_cutoff),
                [0, 1, 0, 0.8],
                line_thickness, 0.15, True
            )
        except Exception:
            print("ì‹œê°í™” ì˜¤ë¥˜ ë°œìƒ")
            pass

    # ======================================================================
    # ë³´ìƒ/ì¢…ë£Œ ê´€ë ¨
    # ======================================================================
    def _formation_reward(self, agent_pos, leader_pos):
        rel = leader_pos - agent_pos
        dist = math.hypot(float(rel[0]), float(rel[1]))
        if dist < 0.5 or dist > 60.0:
            return -5.0
        ideal = 10.0
        sigma = 10.0
        r = 3.0 * math.exp(-((dist - ideal) ** 2) / (2.0 * sigma ** 2)) - 1.0
        return r

    def _guardian_reward(self, agent_pos, leader_pos, dynamic_pos):
        d_lo = np.linalg.norm(leader_pos[:2] - dynamic_pos[:2])
        d_ao = np.linalg.norm(agent_pos[:2] - dynamic_pos[:2])

        ALERT_DIST = 80.0
        if d_lo > ALERT_DIST:
            return 0.0

        if d_ao < d_lo:
            score = (d_lo - d_ao) / max(d_lo, 1e-3)
            return 2.0 * score
        else:
            return -0.5


    def _compute_reward(self, agent, distance_leader, distance_other, distance_dynamic):        
        """
        ë¡¤ ê¸°ë°˜ ë³´ìƒ ì„¤ê³„ (ìˆ˜ì •ë¨):
        - Interceptor: PN ìœ ë„(ì ‘ê·¼, LOS) + â˜…Heading Alignment(ì •ë©´ ì£¼ì‹œ) ë³´ìƒ ì¶”ê°€
        - Defender: ìœ ì¸ê¸° ë³´í˜¸ ìœ ì§€
        """

        eps = 1e-6

        # -----------------------------------------------------
        # 0. í˜„ì¬ ìœ„ì¹˜ ìºì‹œì—ì„œ ë¦¬ë”/ì /ì—ì´ì „íŠ¸ ìœ„ì¹˜ ë°›ì•„ì˜¤ê¸°
        # -----------------------------------------------------
        enemy_pose = self.current_location[self.dynamic_name].position
        enemy_xy = np.array([enemy_pose.x_val, enemy_pose.y_val], dtype=np.float32)

        leader_pose = self.current_location["Drone1"].position
        leader_xy = np.array([leader_pose.x_val, leader_pose.y_val], dtype=np.float32)

        # Interceptor ì„ ì • (ê°€ì¥ ê°€ê¹Œìš´ ì—ì´ì „íŠ¸)
        min_agent = None
        min_dist = float("inf")
        for ag in self.possible_agents:
            pose = self.current_location.get(ag, None)
            if pose is None:
                continue
            ag_xy = np.array([pose.position.x_val, pose.position.y_val], dtype=np.float32)
            d = float(np.linalg.norm(ag_xy - enemy_xy))
            if d < min_dist:
                min_dist = d
                min_agent = ag

        interceptor = min_agent
        is_interceptor = (agent == interceptor)

        # -----------------------------------------------------
        # 1. ë¬¼ë¦¬ëŸ‰ ê°€ì ¸ì˜¤ê¸° (PN ê³„ì‚°ìš©)
        # -----------------------------------------------------
        my_state = self.client.getMultirotorState(vehicle_name=agent)
        my_pos = np.array([
            my_state.kinematics_estimated.position.x_val,
            my_state.kinematics_estimated.position.y_val,
        ], dtype=np.float32)
        my_vel = np.array([
            my_state.kinematics_estimated.linear_velocity.x_val,
            my_state.kinematics_estimated.linear_velocity.y_val,
        ], dtype=np.float32)
        
        # â˜… [ì¶”ê°€] ë‚˜ì˜ í—¤ë”© ë²¡í„° ê³„ì‚° (ë°”ë¼ë³´ëŠ” ë°©í–¥)
        my_yaw = self.heading_state[agent]
        my_heading_vec = np.array([math.cos(my_yaw), math.sin(my_yaw)], dtype=np.float32)

        target_state = self.client.getMultirotorState(vehicle_name=self.dynamic_name)
        target_pos = np.array([
            target_state.kinematics_estimated.position.x_val,
            target_state.kinematics_estimated.position.y_val,
        ], dtype=np.float32)
        target_vel = np.array([
            target_state.kinematics_estimated.linear_velocity.x_val,
            target_state.kinematics_estimated.linear_velocity.y_val,
        ], dtype=np.float32)

        # ìƒëŒ€ ìœ„ì¹˜ / ì†ë„
        R_vec = target_pos - my_pos   # ì—ì´ì „íŠ¸ -> ì  ë²¡í„°
        V_vec = target_vel - my_vel   
        dist = float(np.linalg.norm(R_vec))

        # -----------------------------------------------------
        # 2. ìš”ê²© ë¡¤(Interceptor) ë³´ìƒ
        # -----------------------------------------------------
        r_close = 0.0
        r_los   = 0.0
        r_dist  = 0.0
        r_align = 0.0

        if is_interceptor and dist > eps:
            
            # (1) Alignment Reward: ì ì„ ì •ë©´ìœ¼ë¡œ ë°”ë¼ë³´ê³  ìˆëŠ”ê°€?
            # R_vec ì •ê·œí™” (ì  ë°©í–¥ ë‹¨ìœ„ ë²¡í„°)
            to_target_unit = R_vec / dist
            
            # ë‚´ì : 1.0(ì •ë©´), 0.0(ì¸¡ë©´), -1.0(ë°˜ëŒ€)
            align_score = float(np.dot(my_heading_vec, to_target_unit))
            
            # ì ì„ ë°”ë¼ë³¼ ë•Œë§Œ ë³´ìƒ ë¶€ì—¬ (ê°€ì¤‘ì¹˜ 2.0)
            if align_score > 0:
                r_align = 2.0 * align_score

            # (2) Closing Speed ë³´ìƒ (ì¡°ê±´ë¶€ ì ìš©)
            closing_speed = -float(np.dot(R_vec, V_vec)) / (dist + eps)
            V_MAX = 30.0
            closing_norm = float(np.clip(closing_speed / V_MAX, -1.0, 1.0))

            if closing_norm > 0.0:
                # â˜… í•µì‹¬: ì ì„ ë°”ë¼ë³´ê³  ìˆì„ ë•Œ(align > 0.5)ë§Œ ì œëŒ€ë¡œ ëœ ì ‘ê·¼ ë³´ìƒ
                # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´(ë“±ì§€ê³  ì ‘ê·¼ ë“±) ë³´ìƒì„ 1/10ë¡œ ì¤„ì„
                if align_score > 0.5:
                    r_close = self.W_CLOSE * closing_norm
                else:
                    r_close = self.W_CLOSE * closing_norm * 0.1

            # (3) LOS-rate ë³´ìƒ
            cross = float(R_vec[0] * V_vec[1] - R_vec[1] * V_vec[0])
            los_rate = cross / (dist**2 + eps)
            LAMBDA_DOT_MAX = 10.0
            los_norm = float(np.clip(abs(los_rate) / LAMBDA_DOT_MAX, 0.0, 1.0))
            
            r_los = self.W_LOS * (1.0 - los_norm)

            # (4) ê±°ë¦¬ ê°œì„  ë³´ìƒ (Delta Distance)
            d_now = dist
            prev_d = self._prev_d_agent_enemy.get(agent, None)
            if prev_d is not None:
                delta = prev_d - d_now
                if delta > 0.0:
                    delta_clipped = float(np.clip(delta, -self.MAX_DIST_DELTA, self.MAX_DIST_DELTA))
                    r_dist = self.W_DIST * (delta_clipped / self.MAX_DIST_DELTA)
            
            # ì´ì „ ê±°ë¦¬ ê°±ì‹ 
            self._prev_d_agent_enemy[agent] = d_now
            
        else:
            # Defenderë„ ê±°ë¦¬ ê¸°ë¡ ê°±ì‹  (ë‚˜ì¤‘ì— ì—­í•  ë°”ë€” ë•Œ íŠ ë°©ì§€)
            self._prev_d_agent_enemy[agent] = dist

        # -----------------------------------------------------
        # 3. ë¹„-ìš”ê²© ë¡¤(Defender): ìœ ì¸ê¸° ë³´í˜¸ ë³´ìƒ
        # -----------------------------------------------------
        r_def = 0.0

        if not is_interceptor:
            d_leader = float(np.linalg.norm(my_pos - leader_xy))
            center = self.optimal_distance
            band   = self.DEF_LEADER_BAND

            offset = abs(d_leader - center)
            if offset < band:
                ratio = 1.0 - (offset / band)
                r_def = self.W_DEF_LEADER * ratio

        # -----------------------------------------------------
        # 4. ìµœì¢… ë³´ìƒ í•©ì‚°
        # -----------------------------------------------------
        # r_alignì´ ì¶”ê°€ë¨
        reward = r_close + r_los + r_dist + r_def + r_align

        return float(reward), False

    
    def _end_episode(self, reward, status):
        """
        ì—í”¼ì†Œë“œ ì¢…ë£Œ í—¬í¼ (ì¶©ëŒ/ì„±ê³µ/íƒ€ì„ì•„ì›ƒ ë“± ì´ë²¤íŠ¸ ë°œìƒ ì‹œ)
        """
        _obs_list = []
        _rewards_list = []
        _terminations_list = []
        _infos_list = []

        # ì—í”¼ì†Œë“œ ë‹¨ìœ„ ì§€í‘œ (0/1 í”Œë˜ê·¸)
        is_success = 1 if status == "SUCCESS_DISTANCE_AGENT_DYNAMIC" else 0

        # ë¦¬ë” í”¼ê²©
        is_leader_hit = 1 if status == "FAIL_LEADER_AND_DYNAMIC_OBSTACLE_COLLISION" else 0

        # ì•„êµ° ì¶©ëŒ: ì—ì´ì „íŠ¸-ì—ì´ì „íŠ¸, ì—ì´ì „íŠ¸-ë¦¬ë” ë‘˜ ë‹¤ í¬í•¨
        is_ally_collision = 1 if status in (
            "FAIL_DISTANCE_AGENT_AGENT",
            "FAIL_DISTANCE_AGENT_LEADER",
        ) else 0

        for agent in self.possible_agents:
            _obs_list.append(self._get_obs(agent))
            _rewards_list.append(reward)
            _terminations_list.append(True)

            _infos_list.append({
                agent: {
                    "final_status": status,
                    "reward": reward,

                    # ì—í”¼ì†Œë“œ ì§€í‘œ (0 ë˜ëŠ” 1)
                    "episode_success": is_success,
                    "episode_leader_hit": is_leader_hit,
                    "episode_ally_collision": is_ally_collision,
                }
            })
        
        return _obs_list, _rewards_list, _terminations_list, _infos_list


    # --------------------- ë™ì ì¥ì• ë¬¼ FSM ---------------------
    def _update_dynamic_obstacle(self):
        """
        ë™ì  ì¥ì• ë¬¼ FSM (Step Count ê¸°ë°˜)
        """
        self._obs_step_timer += 1  # í˜„ì¬ ìƒíƒœì—ì„œì˜ ê²½ê³¼ ìŠ¤í… ì¦ê°€

        # ==========================================
        # STATE: IDLE (ëŒ€ê¸° ìƒíƒœ)
        # ==========================================
        if self._obstacle_state == "IDLE":
            # í˜¸ë²„ë§ ìœ ì§€ (ìœ„ì¹˜ ê³ ì •)
            self.client.moveByVelocityAsync(0, 0, 0, duration=0.1, vehicle_name=self.dynamic_name)

            # [ì¡°ê±´ ì²´í¬] ì •í•´ì§„ ëŒ€ê¸° ìŠ¤í…(10~30)ì´ ì§€ë‚¬ëŠ”ê°€?
            if self._obs_step_timer >= self._idle_wait_steps:
                print(f"[DynamicObstacle] {self._obs_step_timer} steps passed. IDLE -> ATTACK!")
                self._obstacle_state = "ATTACK"
                self._obs_step_timer = 0  # íƒ€ì´ë¨¸ ë¦¬ì…‹
        
        # ==========================================
        # STATE: ATTACK (ì¶”ì  ìƒíƒœ)
        # ==========================================
        elif self._obstacle_state == "ATTACK":
            # 1. ìœ ì¸ê¸° ë°©í–¥ ê³„ì‚°
            try:
                leader_pos = self.current_location["Drone1"].position
                obs_pos = self.current_location[self.dynamic_name].position
                
                l_vec = np.array([leader_pos.x_val, leader_pos.y_val, leader_pos.z_val])
                o_vec = np.array([obs_pos.x_val, obs_pos.y_val, obs_pos.z_val])
                
                diff = l_vec - o_vec
                dist = np.linalg.norm(diff)
                
                if dist > 0.5:
                    direction = diff / dist
                    speed = 2.0  # ê³µê²© ì†ë„ (m/s)
                    vel = direction * speed
                    
                    # 2. ì†ë„ ëª…ë ¹ ì „ì†¡ (ìœ ë„íƒ„ ì²˜ëŸ¼ ì¶”ì )
                    self.client.moveByVelocityAsync(
                        vx=float(vel[0]), vy=float(vel[1]), vz=float(vel[2]), 
                        duration=0.1, 
                        vehicle_name=self.dynamic_name
                    )
            except Exception as e:
                print(f"Attack Logic Error: {e}")

            # 3. [ì•ˆì „ ì¥ì¹˜] ë„ˆë¬´ ì˜¤ë«ë™ì•ˆ(ì˜ˆ: 500ìŠ¤í…) ëª» ë§ì¶”ë©´ ê°•ì œ ë¦¬ì…‹ (ë¬´í•œ ì¶”ì  ë°©ì§€)
            if self._obs_step_timer > 500:
                 print("[DynamicObstacle] Attack Timeout. Forcing Reset.")
                 self._reset_obstacle_logic()


    def _teleport_obstacle_randomly(self):
        """ì¥ì• ë¬¼ì„ ë¦¬ë” ê·¼ì²˜ ëœë¤ ìœ„ì¹˜ë¡œ ìˆœê°„ì´ë™ ì‹œí‚´"""
        leader_pos = self.client.simGetObjectPose("Drone1").position
        lx, ly = leader_pos.x_val, leader_pos.y_val
        
        # 50m ~ 60m ë°˜ê²½ ë‚´ ëœë¤ ìœ„ì¹˜
        radius = random.uniform(50.0, 60.0)
        angle = random.uniform(0, 2 * math.pi)
        
        tx = lx + radius * math.cos(angle)
        ty = ly + radius * math.sin(angle)
        tz = self.fixed_z

        # ìœ„ì¹˜ ê°•ì œ ì„¤ì •
        pose = airsim.Pose(airsim.Vector3r(tx, ty, tz), airsim.Quaternionr(0,0,0,1))
        self.client.simSetVehiclePose(pose, True, vehicle_name=self.dynamic_name)

        
        # ì¤‘ìš”: ìˆœê°„ì´ë™ í›„ ì´ì „ ì†ë„(ê´€ì„±) ì œê±°
        self.client.moveByVelocityAsync(0,0,0, duration=0.1, vehicle_name=self.dynamic_name).join()

    def _reset_obstacle_logic(self):
        """
        ê³µê²© ì™„ë£Œ/ì‹¤íŒ¨ í›„ í˜¸ì¶œ:
        1. ëœë¤ ìœ„ì¹˜ë¡œ ìˆœê°„ì´ë™
        2. ìƒíƒœë¥¼ IDLEë¡œ ë³€ê²½
        3. ëœë¤ ëŒ€ê¸° ì‹œê°„(10~30 step) ì¬ì„¤ì •
        """
        self._teleport_obstacle_randomly()
        
        self._obstacle_state = "IDLE"
        self._obs_step_timer = 0
        self._idle_wait_steps = random.randint(10, 30) # ë‹¤ìŒ ëŒ€ê¸° ì‹œê°„ ëœë¤ ì„¤ì •
        
        print(f"[DynamicObstacle] Reset to IDLE. Waiting for {self._idle_wait_steps} steps.")
    
    def _check_distance_collision(self, name_a, name_b, threshold):
        pa = self.current_location[name_a].position
        pb = self.current_location[name_b].position

        dx = pa.x_val - pb.x_val
        dy = pa.y_val - pb.y_val

        dist = math.sqrt(dx*dx + dy*dy)
        return dist < threshold, dist


    # ======================================================================
    # RL/PettingZoo API
    # ======================================================================
    @property
    def observation_space(self):
        return [self.observation_spaces[a] for a in self.possible_agents]

    @property
    def action_space(self):
        return [self.action_spaces[a] for a in self.possible_agents]

    @property
    def share_observation_space(self):
        return [self.share_observation_spaces for _ in self.possible_agents]

    def seed(self, seed):
        random.seed(seed)
        np.random.seed(seed)

    def _get_obs(self, agent):
        # ---------------------------------------------------------
        # 1. ì—ì´ì „íŠ¸(ë‚˜)ì˜ ìš´ë™ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (World Frame)
        # ---------------------------------------------------------
        my_state = self.client.getMultirotorState(vehicle_name=agent)
        my_pos = np.array([
            my_state.kinematics_estimated.position.x_val,
            my_state.kinematics_estimated.position.y_val
        ], dtype=np.float32)
        my_vel = np.array([
            my_state.kinematics_estimated.linear_velocity.x_val,
            my_state.kinematics_estimated.linear_velocity.y_val
        ], dtype=np.float32)

        # ---------------------------------------------------------
        # 2. ë™ì  ì¥ì• ë¬¼(ì )ì˜ ìš´ë™ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        # ---------------------------------------------------------
        target_state = self.client.getMultirotorState(vehicle_name=self.dynamic_name)
        target_pos = np.array([
            target_state.kinematics_estimated.position.x_val,
            target_state.kinematics_estimated.position.y_val
        ], dtype=np.float32)
        target_vel = np.array([
            target_state.kinematics_estimated.linear_velocity.x_val,
            target_state.kinematics_estimated.linear_velocity.y_val
        ], dtype=np.float32)

        # ---------------------------------------------------------
        # 3. ìœ ë„íƒ„ ê³µí•™ í•„ìˆ˜ ë¬¼ë¦¬ëŸ‰ ê³„ì‚° (Intercept Physics)
        # ---------------------------------------------------------
        R_vec = target_pos - my_pos     # ìƒëŒ€ ìœ„ì¹˜
        V_vec = target_vel - my_vel     # ìƒëŒ€ ì†ë„
        
        dist = float(np.linalg.norm(R_vec))
        epsilon = 1e-6  # 0 ë‚˜ëˆ„ê¸° ë°©ì§€

        # (1) Closing Velocity: - (RÂ·V) / |R|
        closing_speed = -float(np.dot(R_vec, V_vec)) / (dist + epsilon)

        # (2) LOS Rate: (R x V) / |R|^2  (2D ì™¸ì )
        cross_prod = float(R_vec[0] * V_vec[1] - R_vec[1] * V_vec[0])
        los_rate = cross_prod / (dist**2 + epsilon)

        # ---- ë¬¼ë¦¬ì ìœ¼ë¡œ ë§ì´ ì•ˆ ë˜ê²Œ íŠ€ëŠ” ê°’ì€ Box ë²”ìœ„ë¡œ í´ë¦¬í•‘ ----
        closing_speed = float(np.clip(closing_speed, -30.0, 30.0))
        los_rate      = float(np.clip(los_rate,      -10.0, 10.0))

        # ---------------------------------------------------------
        # 4. ê´€ì¸¡ê°’ ì¡°ë¦½
        # ---------------------------------------------------------
        _leader_feats  = []
        _ally_feats    = []
        _dynamic_feats = []

        # ë¦¬ë”: (ìƒëŒ€ ë°©ìœ„, ê±°ë¦¬)
        _leader_feats = [self._angle_and_distance(agent, "Drone1")]

        # ì•„êµ°ë“¤: (ìƒëŒ€ ë°©ìœ„, ê±°ë¦¬) * N
        other_agents = [a for a in self.possible_agents if a != agent]
        for other in other_agents:
            _ally_feats.append(self._angle_and_distance(agent, other))

        # ì : (ìƒëŒ€ ë°©ìœ„, ê±°ë¦¬) + (closing_speed, los_rate)
        base_dynamic_feat = self._angle_and_distance(agent, self.dynamic_name)  # (angle, dist)

        _dynamic_feats = [
            base_dynamic_feat[0],  # ìƒëŒ€ ë°©ìœ„
            base_dynamic_feat[1],  # ìƒëŒ€ ê±°ë¦¬
            closing_speed,         # ì ‘ê·¼ ì†ë„
            los_rate               # ì‹œì„ ê° ë³€í™”ìœ¨
        ]

        # [ë¦¬ë”(2), ì•„êµ°*(2), ì (4)] â†’ ì´ 10ì°¨ì›
        obs = np.concatenate([
            np.array(_leader_feats,  dtype=np.float32).flatten(),
            np.array(_ally_feats,    dtype=np.float32).flatten(),
            np.array(_dynamic_feats, dtype=np.float32).flatten(),
        ]).astype(np.float32)

        return obs


    def _do_action(self, actions):
        """
        heading_state + speed ë°©ì‹ ì œì–´
        -------------------------------------
        actions[i][0] = yaw_rate  [-1, 1] â†’ [-MAX_YAW, MAX_YAW]
        actions[i][1] = speed     [-1, 1] â†’ [0, MAX_SPEED]

        heading_state[agent]ì€ í™˜ê²½ ë‚´ë¶€ì—ì„œ ëˆ„ì  ê´€ë¦¬.
        """

        actions = np.clip(actions, -1, 1)

        dt = 0.1  # step interval (durationê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)

        for i, agent in enumerate(self.possible_agents):

            a = actions[i]

            # ===============================
            # 1) yaw_rate ëˆ„ì  â†’ heading ì—…ë°ì´íŠ¸
            # ===============================
            yaw_rate = a[0] * self.MAX_YAW
            self.heading_state[agent] += yaw_rate * dt  # ë‚´ë¶€ yaw ìƒíƒœ ì—…ë°ì´íŠ¸

            # heading ì •ê·œí™”
            self.heading_state[agent] = (self.heading_state[agent] + math.pi) % (2 * math.pi) - math.pi

            # ===============================
            # 2) speed ë³€í™˜
            # ===============================
            speed = (a[1] + 1) / 2 * self.MAX_SPEED  # [0, MAX_SPEED]

            # ===============================
            # 3) heading ê¸°ë°˜ velocity ê³„ì‚°
            # ===============================
            yaw = self.heading_state[agent]

            vx = math.cos(yaw) * speed
            vy = math.sin(yaw) * speed

            # ===============================
            # 4) velocity ëª…ë ¹ ì „ì†¡ (ê³ ë„ ìœ ì§€)
            # ===============================
            self.client.moveByVelocityZAsync(
                vx=vx,
                vy=vy,
                z=self.fixed_z,
                duration=dt,
                vehicle_name=agent
            )

    
    def _get_rewards(self, per_agent_results):
        return [np.mean(per_agent_results) for _ in self.possible_agents]


    def reset(self, seed=None, options=None):
        self.episode_count += 1
        print(f"Episode: {self.episode_count}")

        self.step_count = 0
        self.agents = self.possible_agents[:]
        self.heading_state = {a: 0.0 for a in self.possible_agents}

        # 1. ì›”ë“œ ë¦¬ì…‹ & ì´ë¥™
        self.client.reset()
        self._setup_flight() 
        self.client.simFlushPersistentMarkers()

        # ---------------------------------------------------------
        # [Step 1] ì (Enemy) ìœ„ì¹˜ ë°°ì • ë° í…”ë ˆí¬íŠ¸
        # ---------------------------------------------------------
        self._reset_obstacle_logic()
        enemy_pos = self.client.simGetObjectPose(self.dynamic_name).position

        # ---------------------------------------------------------
        # [Step 2] ìœ ì¸ê¸°(Leader) ìœ„ì¹˜ ê³ ì • (ì¶”ë½ ë°©ì§€!)
        # ---------------------------------------------------------
        # 1. ìœ„ì¹˜ ê°•ì œ ì„¤ì • (0,0, -10m)
        leader_pose = airsim.Pose(airsim.Vector3r(0, 0, self.fixed_z), airsim.Quaternionr(0,0,0,1))
        self.client.simSetVehiclePose(leader_pose, True, vehicle_name="Drone1")
        
        # [â˜…ì¶”ê°€] í…”ë ˆí¬íŠ¸ ì§í›„ ì¶”ë½í•˜ì§€ ì•Šê²Œ ì¦‰ì‹œ í˜¸ë²„ë§ ëª…ë ¹ ì „ì†¡
        self.client.moveByVelocityZAsync(0, 0, self.fixed_z, duration=0.1, vehicle_name="Drone1")

        # ---------------------------------------------------------
        # [Step 3] ì•„êµ°(Agent) ìœ„ì¹˜ ê³„ì‚° ë° í…”ë ˆí¬íŠ¸
        # ---------------------------------------------------------
        radius = 5.0
        num_agents = len(self.possible_agents)
        sector_angle = (2 * math.pi) / num_agents

        for i, agent in enumerate(self.possible_agents):
            
            # A. ìœ„ì¹˜ ê³„ì‚°
            base_angle = i * sector_angle
            jitter = random.uniform(-math.radians(30), math.radians(30))
            final_angle = base_angle + jitter
            
            target_x = radius * math.cos(final_angle)
            target_y = radius * math.sin(final_angle)
            
            # B. ì ì„ ë°”ë¼ë³´ëŠ” ê°ë„ ê³„ì‚°
            dx = enemy_pos.x_val - target_x
            dy = enemy_pos.y_val - target_y
            target_yaw = math.atan2(dy, dx)
            
            # C. Pose ìƒì„± ë° ì ìš©
            start_pos = airsim.Vector3r(target_x, target_y, self.fixed_z)
            start_rot = airsim.to_quaternion(0, 0, target_yaw)
            start_pose = airsim.Pose(start_pos, start_rot)
            
            self.client.simSetVehiclePose(start_pose, True, vehicle_name=agent)
            
            # ë‚´ë¶€ ë³€ìˆ˜ ë™ê¸°í™”
            self.heading_state[agent] = target_yaw
            
            # ê´€ì„± ì œê±° ëª…ë ¹
            self.client.moveByVelocityAsync(0, 0, 0, duration=0.1, vehicle_name=agent)

        # ---------------------------------------------------------
        
        # ë¬¼ë¦¬ ì•ˆì •í™” ëŒ€ê¸° (ì´ë•Œ ìœ ì¸ê¸°ê°€ ì¶”ë½í•˜ì§€ ì•Šë„ë¡ ìœ„ì—ì„œ ëª…ë ¹ì„ ì¤¬ìŒ)
        time.sleep(0.5)
        
        # [â˜…í™•ì¸ì‚¬ì‚´] ì‹œì‘ ì „ ì „ì²´ ë“œë¡  ë‹¤ì‹œ í•œ ë²ˆ í˜¸ë²„ë§ ê³ ì •
        self._hover("Drone1")
        for agent in self.possible_agents:
            self._hover(agent)

        # ê° ë“œë¡  í˜„ì¬ ìœ„ì¹˜/ìì„¸ ê°±ì‹ 
        self._get_current_location()

        # ë³€ìˆ˜ ì´ˆê¸°í™”
        self.step_count = 0
        self._last_pose.clear()
        self._last_time.clear()
        self._last_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self.leader_stop = False
        self._prev_d_leader_enemy = None
        self._prev_d_agent_enemy = {a: None for a in self.possible_agents}
        self._prev_los_angle = {a: None for a in self.possible_agents}

        obs_list = [self._get_obs(a) for a in self.agents]
        print("Reset complete: Leader is hovering, Agents ready.")

        return obs_list

    def step(self, actions):

        # ===== ìŠ¤í… ì‹œ ì´ˆê¸°í™” ì¸ìŠ¤í„´ìŠ¤ë“¤ ===== 
        # - ìŠ¤í… ì¹´ìš´ë“œ +1
        self.step_count += 1
        # - ì—ì´ì „íŠ¸ ë³„ | ê´€ì¸¡ / ë³´ìƒ / ì •ë³´ | ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        per_agent_obs, per_agent_results, per_agent_infos = [], [], []


        # ===== Action Step =====
        # - ì—ì´ì „íŠ¸ ì•¡ì…˜ ì ìš© (ì—ì´ì „íŠ¸ë³„ action êµ¬í˜„)
        self._do_action(actions)

        # - ìœ ì¸ê¸°/ì¥ì• ë¬¼ ì´ë™
        self._update_leader_movement()        
        self._update_dynamic_obstacle()

        # í˜„ì¬ ìœ„ì¹˜ ê°’ ë°›ì•„ì˜¤ê¸° (World Frame)
        self._get_current_location()
        #print(self.current_location["Follower1"].orientation)

        # ===== Check Termination Step =====
        for agent in self.possible_agents:
            
            # ì´ë²ˆ ìŠ¤í…ì— í™œìš©í•  ê±°ë¦¬ ì •ë³´ ë°ì´í„° ë¯¸ë¦¬ ì—°ì‚° í•´ë‘ê¸° (ê° ì—ì´ì „íŠ¸ ë³„ ê¸°ì¤€)
            other_agents = [a for a in self.possible_agents if a != agent]  # ë³¸ì¸ì´ ì•„ë‹Œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ ë°°ì—´ ë¶ˆëŸ¬ì˜¤ê¸°  

            ## - ìœ ì¸ê¸°ì™€ì˜ ê±°ë¦¬
            _distance_leader = np.linalg.norm([
                self.current_location[agent].position.x_val - self.current_location["Drone1"].position.x_val,
                self.current_location[agent].position.y_val - self.current_location["Drone1"].position.y_val,
                self.current_location[agent].position.z_val - self.current_location["Drone1"].position.z_val
            ])
            ## - ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ ê°„ì˜ ê±°ë¦¬ (List)
            _distance_other = [
                np.linalg.norm([
                    self.current_location[agent].position.x_val - self.current_location[other].position.x_val,
                    self.current_location[agent].position.y_val - self.current_location[other].position.y_val,
                    self.current_location[agent].position.z_val - self.current_location[other].position.z_val
                ]) for other in other_agents
            ]
            ## - ë™ì  ì¥ì• ë¬¼ê³¼ì˜ ê±°ë¦¬
            _distance_dynamic = np.linalg.norm([
                self.current_location[agent].position.x_val - self.current_location[self.dynamic_name].position.x_val,
                self.current_location[agent].position.y_val - self.current_location[self.dynamic_name].position.y_val,
                self.current_location[agent].position.z_val - self.current_location[self.dynamic_name].position.z_val
            ])

            # - ë§Œì•½ ì—ì´ì „íŠ¸ê°€ ìœ ì¸ê¸°ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ì„ ê²½ìš°,
            if _distance_leader > self.far_cutoff:
                print(
                    f"[ì´íƒˆ] {agent}ê°€ ë¦¬ë”ì™€ì˜ ê±°ë¦¬({_distance_leader:.2f}m)ë¡œ, "
                    f"ì´íƒˆ ì„ê³„ê°’({self.far_cutoff}m) ì´ˆê³¼! â†’ ì „ì²´ ì‹¤íŒ¨(ê²½ê³„ ì´íƒˆ)"
                )
                return self._end_episode(self.REWARD_AGENT_CRASH,"FAIL_AGENT_FAR_CUTOFF")

            
            # - ë§Œì•½ ì—ì´ì „íŠ¸ê°€ ìœ ì¸ê¸°ì™€ ì¶©ëŒí–ˆì„ ê²½ìš°,
            hit, d = self._check_distance_collision(agent, "Drone1", threshold=self.COLLISION_THRESHOLD)
            if hit:
                print(f"âš ï¸ğŸ’”[ê±°ë¦¬-ì¶©ëŒ] {agent} â†” Drone1  (d={d:.2f}) â†’ ì „ì²´ ì‹¤íŒ¨")
                return self._end_episode(self.REWARD_AGENT_CRASH, "FAIL_DISTANCE_AGENT_LEADER")
            
            # - ë§Œì•½ ì—ì´ì „íŠ¸ê°€ ì—ì´ì „íŠ¸ì™€ ì¶©ëŒí–ˆì„ ê²½ìš°,
            for other in other_agents:
                hit, d = self._check_distance_collision(agent, other, threshold=self.COLLISION_THRESHOLD)
                if hit:
                    print(f"ğŸ’¥ğŸ¤–[ê±°ë¦¬-ì¶©ëŒ] {agent} â†” {other}  (d={d:.2f}) â†’ ì „ì²´ ì‹¤íŒ¨")
                    return self._end_episode(self.REWARD_AGENT_CRASH, "FAIL_DISTANCE_AGENT_AGENT")
            
            # - ë§Œì•½ ìœ ì¸ê¸°ê°€ ë™ì ì¥ì• ë¬¼ê³¼ ì¶©ëŒí–ˆì„ ê²½ìš°,
            collisionInfo = self.client.simGetCollisionInfo("Drone1")
            if collisionInfo.has_collided and collisionInfo.object_name == self.dynamic_name:
                print(f"ğŸ’¥[ì¶©ëŒ] ìœ ì¸ê¸°ê°€ {collisionInfo.object_name}ì™€ ì¶©ëŒë¡œ â†’ ì „ì²´ ì‹¤íŒ¨")
                return self._end_episode(self.REWARD_LEADER_HIT, "FAIL_LEADER_AND_DYNAMIC_OBSTACLE_COLLISION")

            # - ë§Œì•½ ì—ì´ì „íŠ¸ê°€ ë™ì  ì¥ì• ë¬¼ê³¼ ì¶©ëŒí–ˆì„ ê²½ìš° (ìš”ê²© ì„±ê³µ),
            hit, d = self._check_distance_collision(agent, self.dynamic_name, threshold=self.COLLISION_THRESHOLD)
            if hit:
                print(f"ğŸ¯ğŸ”¥[ê±°ë¦¬-ì¶©ëŒ] {agent} â†” {self.dynamic_name}  (d={d:.2f}) â†’ ìš”ê²© ì„±ê³µ")
                return self._end_episode(self.REWARD_HIT_ENEMY, "SUCCESS_DISTANCE_AGENT_DYNAMIC")


            # - ì¢…ë£Œ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ëª»í•œ ê²½ìš°,
            per_agent_obs.append(self._get_obs(agent))

            _reward, _ = self._compute_reward(agent, _distance_leader, _distance_other, _distance_dynamic)
            per_agent_results.append(_reward)
            per_agent_infos.append([f"reward: {_reward}"])

        # ë„ì¤‘ì— ì¢…ë£Œ ì•ˆë˜ë©´ ë‹¤ ì¢…ë£Œ ì•ˆí•¨.
        termination_list = [False for _ in self.possible_agents]

        # ===== Rewards Step =====
        rewards_list = self._get_rewards(per_agent_results)

        
        # ===== Observations Step =====
        obs_list = per_agent_obs


        # ===== Infos Step =====
        infos_list = per_agent_infos

        return obs_list, rewards_list, termination_list, infos_list