import gymnasium as gym
import numpy as np
import airsim
import math
import time
import random
from collections import deque


class AirSimMultiDroneEnv:
    metadata = {"render_modes": []}

    def __init__(
        self,
        ip_address="127.0.0.1",
        follower_names=("Follower0", "Follower1", "Follower2"),
        port=41451,
        step_length=0.01,
        leader_velocity=1.0,
        optimal_distance=10.0,
        far_cutoff=60.0,
        too_close=0.5,
        dt=0.01,
        do_visualize=True
    ):
        super().__init__()
        self.possible_agents = list(follower_names)
        self.agents = self.possible_agents[:]

        # ì¶©ëŒ ê´€ë ¨ ì„¤ì •
        self.COLLISION_THRESHOLD = 1.5
        self.STOP_DISTANCE_LEADER_OBSTACLE = 1.0

        # ì†ë„/ì•¡ì…˜ ë²„í¼
        self.vmax_self = 2.0
        self._timestep = float(dt)

        # ì—ì´ì „íŠ¸/ë¦¬ë” ì†ë„ ì‚°ì¶œìš© ë²„í¼
        self._last_pose = {}
        self._last_time = {}

        self._last_action = {a: np.zeros(3, dtype=np.float32) for a in self.possible_agents}
        self._current_action = {a: np.zeros(3, dtype=np.float32) for a in self.possible_agents}
        self.success_steps = {a: 0 for a in self.possible_agents}

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°/í™˜ê²½ íŒŒë¼ë¯¸í„°
        self.step_length = float(step_length)
        self.fixed_z = -10.0
        self.dt = float(dt)
        self.do_visualize = bool(do_visualize)
        self.max_cmd_speed = self.step_length / self.dt
        self.leader_velocity = float(leader_velocity)
        self.optimal_distance = float(optimal_distance)
        self.far_cutoff = float(far_cutoff)
        self.too_close = float(too_close)
        self.follower_names = list(follower_names)
        # Lidar ë©”ëª¨ë¦¬: { "AgentName": { "TargetName": [dx, dy] } }
        self.lidar_memory = {}
        self.prev_lidar_pos = {}
        # [ì¶”ê°€] Lidar ì„¼ì„œ ì´ë¦„ ë§¤í•‘ (settings.jsonì˜ ì´ë¦„ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)
        self.lidar_names = {
            agent: f"{agent}_LidarSensor" for agent in self.possible_agents
        }

        # States
        self.step_count = 0
        self.episode_count = 0

        # í†µê³„ ê³„ì‚°ì„ ìœ„í•œ Deque
        self.stats_history = {
            "win": deque(maxlen=20),
            "coll_leader": deque(maxlen=20),
            "coll_drone": deque(maxlen=20),
            "coll_obj": deque(maxlen=20)
        }

        # ë™ì  ì¥ì• ë¬¼ (ì´ˆê¸°ê°’)
        self.dynamic_name = "DynamicObstacle" 
        self.enemy_names = [] # reset ì‹œ ìë™ ìŠ¤ìº”

        # ë™ì  ì¥ì• ë¬¼ ê´€ë ¨
        self.isIdle = None
        self.D_O_STATE = {0: "idle", 1: "attack"}

        # ===== obs / act / share_obs spaces =====
        self.K_ally = 5   
        self.K_enemy = 5
        self.num_ally = self.K_ally
        self.num_enemy = self.K_enemy

        # ---- [ìˆ˜ì •] ê´€ì¸¡ ê³µê°„ ë²”ìœ„ ì •ì˜ (ìƒëŒ€ ì¢Œí‘œ ë„ì…) ----
        # ê¸°ì¡´: Bearing(-1~1), Dist(0~200)
        # ë³€ê²½: Rel_X(-1~1), Rel_Y(-1~1) ë¡œ ë³€ê²½ (100m ê¸°ì¤€ ì •ê·œí™”)
        
        low_rel_pos = -1.0;        high_rel_pos = 1.0       
        low_vel = -1.0; high_vel = 1.0
        low_rate = -1.0; high_rate = 1.0
        low_self_state = -1.0; high_self_state = 1.0

        # [ë¦¬ë”(2)] + [ì•„êµ°(2)*K] + [ì (4)*K] + [self_state(2)]
        # ì (Enemy)ì˜ ê²½ìš°: [rel_x, rel_y, closing_speed, los_rate]
        per_agent_low = (
            [low_rel_pos, low_rel_pos] +                          # Leader (dx, dy)
            [low_rel_pos, low_rel_pos] * self.num_ally +          # Allies (dx, dy)
            [low_rel_pos, low_rel_pos, low_vel, low_rate] * self.num_enemy + # Enemy
            [low_self_state] * 2                                  # Self (vx, vy)
        )
        per_agent_high = (
            [high_rel_pos, high_rel_pos] +
            [high_rel_pos, high_rel_pos] * self.num_ally +
            [high_rel_pos, high_rel_pos, high_vel, high_rate] * self.num_enemy +
            [high_self_state] * 2
        )

        obs_dim = len(per_agent_low)
        share_obs_dim = obs_dim * len(self.possible_agents)

        self.observation_spaces = {
            agent: gym.spaces.Box(
                low=np.array(per_agent_low, dtype=np.float32),
                high=np.array(per_agent_high, dtype=np.float32),
                shape=(obs_dim,),
                dtype=np.float32,
            )
            for agent in self.possible_agents
        }

        self.MAX_YAW = math.radians(90)
        self.MAX_SPEED = 10

        self.action_spaces = {
            agent: gym.spaces.Box(
                low=-1.0,
                high=1.0,
                shape=(2,),
                dtype=np.float64,
            )
            for agent in self.possible_agents
        }

        self.share_observation_spaces = gym.spaces.Box(
            low=np.array(per_agent_low * len(self.possible_agents), dtype=np.float32),
            high=np.array(per_agent_high * len(self.possible_agents), dtype=np.float32),
            shape=(share_obs_dim,),
            dtype=np.float32,
        )

        # PN ë³´ìƒìš© ë²„í¼ë“¤
        self._prev_d_leader_enemy = None
        self._prev_d_agent_enemy = {a: None for a in self.possible_agents}
        self._prev_los_angle = {a: None for a in self.possible_agents}

        # PN-style reward parameters
        self.REWARD_HIT_ENEMY = 100.0   # ì„±ê³µ ë³´ìƒì€ í¬ê²Œ
        self.REWARD_LEADER_HIT = -250.0
        self.REWARD_AGENT_CRASH = -500.0 # [ì¤‘ìš”] ìì‚´ ë°©ì§€ ìœ„í•´ ì¶©ëŒ íŒ¨ë„í‹° ëŒ€í­ ê°•í™”

        self.W_CLOSE = 0.5
        self.W_LOS = 0.5
        self.W_DIST = 0.5
        self.W_ALLY = 0.5
        self.STEP_PENALTY = 0.0 # [ì¤‘ìš”] ì‹œê°„ íŒ¨ë„í‹° ì œê±° (ì˜¤ë˜ ì‚´ì•„ë‚¨ì•„ ìš”ê²©í•˜ë„ë¡)

        self.MAX_DIST_DELTA = 5.0
        self.MAX_ANGLE_DELTA = math.radians(20.0)
        self.ALLY_SAFE_DIST = 5.0

        # ìœ„ì¹˜ ìºì‹œ
        self.start_location = {}
        self.current_location = {}

        # í´ë¼ì´ì–¸íŠ¸ ì…‹ì—…
        self.client = airsim.MultirotorClient(ip=ip_address, port=port)
        self.client.confirmConnection()

        self._last_visualize_t = time.time()

    def _get_lidar_measurement(self, agent_name, target_name):
        """
        Lidarë¡œ íƒ€ê²Ÿì„ ê´€ì¸¡í•©ë‹ˆë‹¤.
        - ê°ì§€ ì„±ê³µ: í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í‰ê·  ìœ„ì¹˜ ë°˜í™˜ + ë©”ëª¨ë¦¬ ê°±ì‹ 
        - ê°ì§€ ì‹¤íŒ¨: ë©”ëª¨ë¦¬ì— ì €ì¥ëœ 'ë§ˆì§€ë§‰ ìœ„ì¹˜' ë°˜í™˜
        """
        lidar_name = self.lidar_names[agent_name]
        lidar_data = self.client.getLidarData(lidar_name, vehicle_name=agent_name)
        
        # 1. í¬ì¸íŠ¸ ë°ì´í„° íŒŒì‹±
        # SensorLocalFrame ì‚¬ìš© ì‹œ: points[:, 0]=ì „ë°©(x), points[:, 1]=ìš°ì¸¡(y)
        points = np.array(lidar_data.point_cloud, dtype=np.float32).reshape(-1, 3)
        
        detected = False
        meas_dx, meas_dy = 0.0, 0.0

        if len(points) > 2:
            # 2. ê°ì²´ ì¸ì‹ ì‹œë®¬ë ˆì´ì…˜ (GT ìœ„ì¹˜ ê·¼ì²˜ì˜ í¬ì¸íŠ¸ë§Œ í•„í„°ë§)
            # í˜„ì¬ Yawê°€ 0ìœ¼ë¡œ ê³ ì •ëœ ì‹œë®¬ë ˆì´ì…˜ì´ë¯€ë¡œ, World Frame ì°¨ì´ â‰ˆ Body Frame ì°¨ì´ë¡œ ê·¼ì‚¬ ê°€ëŠ¥
            # (ë§Œì•½ ë“œë¡ ì´ íšŒì „í•œë‹¤ë©´ íšŒì „ í–‰ë ¬ ì ìš© í•„ìš”)
            
            my_pos = self.current_location[agent_name].position
            tgt_pos = self.current_location[target_name].position
            
            gt_dx = tgt_pos.x_val - my_pos.x_val
            gt_dy = tgt_pos.y_val - my_pos.y_val
            
            # Lidar í¬ì¸íŠ¸ë“¤ ì¤‘ ì‹¤ì œ íƒ€ê²Ÿ ìœ„ì¹˜ ë°˜ê²½ 3m ë‚´ì— ìˆëŠ” ì ë§Œ ì¶”ì¶œ
            # (SensorLocalFrameì´ë¯€ë¡œ points ìì²´ê°€ ìƒëŒ€ì¢Œí‘œì„)
            dist_sq = (points[:, 0] - gt_dx)**2 + (points[:, 1] - gt_dy)**2
            mask = dist_sq < (3.0)**2  
            target_points = points[mask]

            if len(target_points) > 0:
                # ê°ì§€ ì„±ê³µ! í¬ì¸íŠ¸ë“¤ì˜ í‰ê· ì„ ì¸¡ì •ê°’ìœ¼ë¡œ ì‚¬ìš©
                mean_pos = np.mean(target_points, axis=0)
                raw_dx = mean_pos[0]
                raw_dy = mean_pos[1]
                
                # ì •ê·œí™” (-1 ~ 1, 100m ê¸°ì¤€)
                norm_dx = np.clip(raw_dx / 100.0, -1.0, 1.0)
                norm_dy = np.clip(raw_dy / 100.0, -1.0, 1.0)
                
                # ë©”ëª¨ë¦¬ ê°±ì‹ 
                self.lidar_memory[agent_name][target_name] = [float(norm_dx), float(norm_dy)]
                detected = True
        
        # 3. ë°˜í™˜ (ê°ì§€í–ˆìœ¼ë©´ ê°±ì‹ ëœ ê°’, ëª»í–ˆìœ¼ë©´ ë©”ëª¨ë¦¬ ê°’)
        final_val = self.lidar_memory[agent_name][target_name]
        
        # ë””ë²„ê¹…ìš© (í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
        # if not detected:
        #    print(f"[{agent_name}] Lost {target_name}! Using Memory: {final_val}")
            
        return final_val[0], final_val[1], detected
    def _calculate_lidar_dynamics(self, agent, target, curr_x, curr_y):
        """
        [í•µì‹¬] GT ì—†ì´ ì˜¤ì§ Lidarì˜ (í˜„ì¬ ìœ„ì¹˜ - ì´ì „ ìœ„ì¹˜) ì°¨ë¶„ìœ¼ë¡œ ì†ë„ ì •ë³´ ê³„ì‚°
        """
        # 1. ì´ì „ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸° (Reset ì‹œ ì´ˆê¸°í™” í•„ìˆ˜)
        if agent not in self.prev_lidar_pos: self.prev_lidar_pos[agent] = {}
        if target not in self.prev_lidar_pos[agent]: self.prev_lidar_pos[agent][target] = [curr_x, curr_y]
        
        prev_x, prev_y = self.prev_lidar_pos[agent][target]
        
        # 2. dt ì²´í¬ (0 ë°©ì§€)
        dt = self.dt if self.dt > 1e-6 else 0.01

        # 3. ìƒëŒ€ ì†ë„ ë²¡í„° ì¶”ì • (Relative Velocity)
        # V_rel = (P_curr - P_prev) / dt
        vx_rel = (curr_x - prev_x) / dt
        vy_rel = (curr_y - prev_y) / dt

        # 4. ê±°ë¦¬ ë° ë²¡í„° ì—°ì‚°
        R_vec = np.array([curr_x, curr_y])   # ìƒëŒ€ ìœ„ì¹˜
        V_vec = np.array([vx_rel, vy_rel])   # ìƒëŒ€ ì†ë„ (ì¶”ì •)
        dist = np.linalg.norm(R_vec) + 1e-6

        # 5. PN ìœ ë„ ë³€ìˆ˜ ê³„ì‚°
        # Closing Speed: ê°€ê¹Œì›Œì§€ë©´ +, ë©€ì–´ì§€ë©´ -
        closing_speed = -float(np.dot(R_vec, V_vec)) / dist
        
        # LOS Rate: ì‹œì„ ê° ë³€í™”ìœ¨
        cross_prod = float(R_vec[0]*V_vec[1] - R_vec[1]*V_vec[0])
        los_rate = cross_prod / (dist**2)
        
        # 6. ì •ê·œí™” (í•™ìŠµìš©)
        # 30m/s, 10rad/sëŠ” ê²½í—˜ì  Max ê°’
        norm_closing = np.clip(closing_speed / 30.0, -1.0, 1.0)
        norm_los = np.clip(los_rate / 10.0, -1.0, 1.0)

        return norm_closing, norm_los
    
    def _get_current_location(self):
        self.current_location = {}
        self.current_location["Drone1"] = self.client.simGetObjectPose("Drone1")
        for agent in self.agents:
            self.current_location[agent] = self.client.simGetObjectPose(agent)
        self.current_location[self.dynamic_name] = self.client.simGetObjectPose(self.dynamic_name)

    # ======================================================================
    # ì´ˆê¸°í™”/ì´ë™/ì‹œê°í™” ê´€ë ¨
    # ======================================================================
    def _hover(self, name):
        self.client.moveByVelocityZAsync(
            vx=0.0, vy=0.0,
            z=self.fixed_z,
            duration=0.3,
            vehicle_name=name
        ).join()

        try:
            self.client.hoverAsync(vehicle_name=name).join()
        except:
            pass

    def _setup_flight(self):
        self.client.enableApiControl(True, vehicle_name="Drone1")
        self.client.armDisarm(True, vehicle_name="Drone1")

        for agent in self.possible_agents:
            self.client.enableApiControl(True, vehicle_name=agent)
            self.client.armDisarm(True, vehicle_name=agent)

        self.client.enableApiControl(True, vehicle_name=self.dynamic_name)
        self.client.armDisarm(True, vehicle_name=self.dynamic_name)

        # 1. ì´ë¥™ (Takeoff) - ì‹œë™ ê±¸ê¸°ìš©
        cmds = []
        cmds.append(self.client.takeoffAsync(vehicle_name="Drone1"))
        for agent in self.possible_agents:
            cmds.append(self.client.takeoffAsync(vehicle_name=agent))
        cmds.append(self.client.takeoffAsync(vehicle_name=self.dynamic_name))

        for c in cmds:
            c.join()

        # 2. [ìˆ˜ì •] ì´ë™(MoveTo) ëŒ€ì‹  í…”ë ˆí¬íŠ¸(SetPose) ì‚¬ìš©
        # ë¬¼ë¦¬ì  ì´ë™ ì—†ì´ ì¢Œí‘œë¥¼ ê°•ì œë¡œ ì°ì–´ë²„ë ¤ì„œ ê´€ì„±ì„ 0ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
        
        # (1) Leader ìœ„ì¹˜ ê³ ì •
        pose_leader = airsim.Pose(airsim.Vector3r(0, 0, self.fixed_z), airsim.Quaternionr(0, 0, 0, 1))
        self.client.simSetVehiclePose(pose_leader, True, vehicle_name="Drone1")
        
        # (2) Followers ìœ„ì¹˜ ê³ ì • (ê²¹ì¹˜ì§€ ì•Šê²Œ ì•½ê°„ ë¶„ì‚°ì‹œì¼œë„ ë¨, ì—¬ê¸°ì„  0,0ìœ¼ë¡œ ëª¨ìŒ)
        # ë§Œì•½ ì—ì´ì „íŠ¸ë¼ë¦¬ ê²¹ì³ì„œ íŠ•ê²¨ë‚˜ê°„ë‹¤ë©´ x, yì— ì•½ê°„ì˜ ì˜¤í”„ì…‹ì„ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
        for i, agent in enumerate(self.possible_agents):
            # ì˜ˆ: ì—ì´ì „íŠ¸ë“¤ì„ ë¦¬ë” ì£¼ë³€ì— ì•½ê°„ ë„ì›Œì„œ ë°°ì¹˜í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
            # offset_x = (i % 2) * 2.0 - 1.0  
            # offset_y = (i // 2) * 2.0 - 1.0
            # pose_agent = airsim.Pose(airsim.Vector3r(offset_x, offset_y, self.fixed_z), airsim.Quaternionr(0, 0, 0, 1))
            
            # í˜„ì¬ëŠ” ëª¨ë‘ 0,0ì— ê²¹ì³ë„ ë¬¼ë¦¬ ì¶©ëŒ ë¬´ì‹œ ì„¤ì •ì´ ë˜ì–´ìˆë‹¤ë©´ ê´œì°®ì§€ë§Œ, ë³´í†µì€ ë„ìš°ëŠ”ê²Œ ì¢‹ìŠµë‹ˆë‹¤.
            # ì¼ë‹¨ ê¸°ì¡´ ì½”ë“œ ì˜ë„ëŒ€ë¡œ 0,0ì— ë‘¡ë‹ˆë‹¤.
            pose_agent = airsim.Pose(airsim.Vector3r(0, 0, self.fixed_z), airsim.Quaternionr(0, 0, 0, 1))
            self.client.simSetVehiclePose(pose_agent, True, vehicle_name=agent)

        # (3) ì (DynamicObstacle) ìœ„ì¹˜ ê³ ì • (ì´ˆê¸°í™”ëŠ” ë‚˜ì¤‘ì— _reset_obstacle_logicì—ì„œ í•˜ê² ì§€ë§Œ ì•ˆì „ìƒ ê³ ì •)
        pose_enemy = airsim.Pose(airsim.Vector3r(0, 0, self.fixed_z), airsim.Quaternionr(0, 0, 0, 1))
        self.client.simSetVehiclePose(pose_enemy, True, vehicle_name=self.dynamic_name)

        # 3. [ì¤‘ìš”] ì†ë„ 0ìœ¼ë¡œ ê°•ì œ ì´ˆê¸°í™” (Momentum Kill)
        # ìœ„ì¹˜ë¥¼ ì˜®ê²¨ë„ ì´ì „ ì†ë„ ë²¡í„°ê°€ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ë®ì–´ì”ë‹ˆë‹¤.
        cmds = []
        cmds.append(self.client.moveByVelocityAsync(0, 0, 0, duration=0.1, vehicle_name="Drone1"))
        for agent in self.possible_agents:
            cmds.append(self.client.moveByVelocityAsync(0, 0, 0, duration=0.1, vehicle_name=agent))
        cmds.append(self.client.moveByVelocityAsync(0, 0, 0, duration=0.1, vehicle_name=self.dynamic_name))
        
        for c in cmds:
            c.join()
            
        time.sleep(0.5) # ë¬¼ë¦¬ ì—”ì§„ ì•ˆì •í™” ëŒ€ê¸°

        # 4. ì´ˆê¸° ìœ„ì¹˜ ì €ì¥
        self.start_location["Drone1"] = self.client.simGetObjectPose("Drone1")
        for agent in self.agents:
            self.start_location[agent] = self.client.simGetObjectPose(agent)
        self.start_location[self.dynamic_name] = self.client.simGetObjectPose(self.dynamic_name)

        # Hover í•¨ìˆ˜ í˜¸ì¶œ (ìµœì¢… ì•ˆì •í™”)
        self._hover("Drone1")
        for agent in self.possible_agents:
            self._hover(agent)
        self._hover(self.dynamic_name)

    def _update_leader_movement(self):
        if self.do_visualize:
            now = time.time()
            if (now - self._last_visualize_t) >= 0.1:
                self.client.simFlushPersistentMarkers()
                self._visualize_circles()
                self._last_visualize_t = now

    def _visualize_circles(self):
        try:
            leader_pos = self.client.simGetObjectPose("Drone1").position
            center = np.array([leader_pos.x_val, leader_pos.y_val, leader_pos.z_val], dtype=float)

            def ring_points(radius, n=36):
                pts = []
                for i in range(n + 1):
                    ang = (i / n) * 2 * np.pi
                    x = center[0] + radius * np.cos(ang)
                    y = center[1] + radius * np.sin(ang)
                    z = center[2]
                    pts.append(airsim.Vector3r(x, y, z))
                return pts

            line_thickness = 20.0
            self.client.simPlotLineStrip(
                ring_points(self.optimal_distance),
                [1, 1, 0, 0.8],
                line_thickness, 0.15, True
            )
            self.client.simPlotLineStrip(
                ring_points(self.far_cutoff),
                [0, 1, 0, 0.8],
                line_thickness, 0.15, True
            )
        except Exception:
            pass

    # ======================================================================
    # ë³´ìƒ/ì¢…ë£Œ ê´€ë ¨
    # ======================================================================
    def _compute_reward(self, agent):
        # íƒ€ê²Ÿ: 0ë²ˆ ì  (ë˜ëŠ” ê°€ì¥ ê°€ê¹Œìš´ ì )
        # ë§Œì•½ ì ì´ ì—†ìœ¼ë©´ ë¦¬ë”ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì¡ìŒ (ë°©ì–´ì½”ë“œ)
        target = self.enemy_names[0] if self.enemy_names else "Drone1"
        
        # 1. ì •ê·œí™”ëœ ì¢Œí‘œ ê°€ì ¸ì˜¤ê¸°
        norm_pos = self.lidar_memory[agent][target]
        
        # [ìˆ˜ì • 1] ë¬¼ë¦¬ëŸ‰ ê³„ì‚°ì„ ìœ„í•´ ë¯¸í„°(meter) ë‹¨ìœ„ë¡œ ë³µì› (x100.0)
        curr_x_meter = norm_pos[0] * 100.0
        curr_y_meter = norm_pos[1] * 100.0
        
        # 2. Lidar ê¸°ë°˜ ë™ì—­í•™(ì†ë„) ê³„ì‚°
        c_speed, l_rate = self._calculate_lidar_dynamics(agent, target, curr_x_meter, curr_y_meter)
        
        # 3. ë³´ìƒ ê³„ì‚°
        r_close = self.W_CLOSE * c_speed if c_speed > 0 else 0.0
        r_los = self.W_LOS * (1.0 - abs(l_rate))
        
        return float(r_close + r_los - self.STEP_PENALTY)

    def _end_episode(self, reward, status):
        _obs_list = []
        _rewards_list = []
        _terminations_list = []
        _infos_list = []

        # [ìˆ˜ì • 1] ì„±ê³µ/ì‹¤íŒ¨ íŒì • ë¡œì§ì„ TTGì™€ ë™ì¼í•˜ê²Œ ìƒì„¸í™”
        is_success = 1.0 if status == "SUCCESS_DISTANCE_AGENT_DYNAMIC" else 0.0
        
        # ë¦¬ë” í”¼ê²© (ì  -> ë¦¬ë”)
        is_leader_hit = 1.0 if "LEADER_AND_DYNAMIC" in status else 0.0
        
        # ì•„êµ° ì¶©ëŒ (ì—ì´ì „íŠ¸ë¼ë¦¬ OR ì—ì´ì „íŠ¸ê°€ ë¦¬ë”ì™€ ì¶©ëŒ)
        # PN ê¸°ì¡´ ë¡œì§ì€ "AGENT_AGENT"ë§Œ ì²´í¬í–ˆìœ¼ë‚˜, ë¦¬ë”ì™€ì˜ ì¶©ëŒë„ ì•„êµ° ì¶©ëŒë¡œ ë³´ëŠ” ê²ƒì´ ë§ìŒ
        if "AGENT_AGENT" in status or "AGENT_LEADER" in status:
            is_ally_collision = 1.0 
        else:
            is_ally_collision = 0.0
            
        # ì´íƒˆ (Far Cutoff)
        is_obj_collision = 1.0 if "FAR_CUTOFF" in status else 0.0

        self.stats_history["win"].append(is_success)
        self.stats_history["coll_leader"].append(is_leader_hit)
        self.stats_history["coll_drone"].append(is_ally_collision)
        self.stats_history["coll_obj"].append(is_obj_collision)

        def get_rate(key):
            if len(self.stats_history[key]) > 0:
                return sum(self.stats_history[key]) / len(self.stats_history[key])
            return 0.0

        win_rate = get_rate("win")

        for agent in self.possible_agents:
            _obs_list.append(self._get_obs(agent))
            _rewards_list.append(reward)
            _terminations_list.append(True)

            # [ìˆ˜ì • 2] info êµ¬ì¡°ë¥¼ { "AgentName": { ... } } í˜•íƒœë¡œ ì¤‘ì²© (TTGì™€ í†µì¼)
            # Runnerê°€ ì´ êµ¬ì¡°ë¥¼ ì¸ì‹í•˜ì—¬ wandb ë¡œê·¸ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
            _infos_list.append({
                agent: {
                    "final_status": status,
                    "episode_success": is_success,
                    "episode_leader_hit": is_leader_hit,
                    "episode_ally_collision": is_ally_collision,
                    "win_rate": win_rate,
                    "cur_episode_steps": self.step_count
                }
            })
        
        print(f"[{self.episode_count} Ep] WinRate: {win_rate:.2f} | Status: {status}")
        return _obs_list, _rewards_list, _terminations_list, _infos_list
    
    # --------------------- ë™ì ì¥ì• ë¬¼ FSM ---------------------
    def _update_dynamic_obstacle(self):
        self._obs_step_timer += 1
        target_speed = 3.0 

        if self._obstacle_state == "IDLE":
            self.client.moveByVelocityAsync(0, 0, 0, duration=0.1, vehicle_name=self.dynamic_name)
            if self._obs_step_timer >= self._idle_wait_steps:
                self._obstacle_state = "ATTACK"
                self._obs_step_timer = 0

        elif self._obstacle_state == "ATTACK":
            try:
                leader_pos = self.current_location["Drone1"].position
                obs_pos = self.current_location[self.dynamic_name].position

                l_vec = np.array([leader_pos.x_val, leader_pos.y_val, leader_pos.z_val])
                o_vec = np.array([obs_pos.x_val, obs_pos.y_val, obs_pos.z_val])

                diff = l_vec - o_vec
                dist = np.linalg.norm(diff)

                if dist > 0.5:
                    direction = diff / dist
                    vel = direction * target_speed

                    self.client.moveByVelocityAsync(
                        vx=float(vel[0]), vy=float(vel[1]), vz=float(vel[2]),
                        duration=0.1,
                        vehicle_name=self.dynamic_name
                    )
            except Exception as e:
                print(f"Attack Logic Error: {e}")

    def _teleport_obstacle_randomly(self):
        leader_pos = self.client.simGetVehiclePose("Drone1").position
        lx, ly = leader_pos.x_val, leader_pos.y_val
        radius = 55.0
        angle = random.uniform(0, 2 * math.pi)

        tx = lx - 20 + radius * math.cos(angle)
        ty = ly - 20 + radius * math.sin(angle)
        tz = self.fixed_z

        pose = airsim.Pose(airsim.Vector3r(tx, ty, tz), airsim.Quaternionr(0, 0, 0, 1))
        self.client.simSetVehiclePose(pose, True, vehicle_name=self.dynamic_name)
        self.client.moveByVelocityAsync(0, 0, 0, duration=0.1, vehicle_name=self.dynamic_name).join()

    def _reset_obstacle_logic(self):
        self._teleport_obstacle_randomly()
        self._obstacle_state = "IDLE"
        self._obs_step_timer = 0
        self._idle_wait_steps = random.randint(10, 30)
        print(f"[DynamicObstacle] Reset to IDLE. Waiting for {self._idle_wait_steps} steps.")

    def _check_distance_collision(self, name_a, name_b, threshold):
        pa = self.current_location[name_a].position
        pb = self.current_location[name_b].position

        dx = pa.x_val - pb.x_val
        dy = pa.y_val - pb.y_val

        dist = math.sqrt(dx * dx + dy * dy)
        return dist < threshold, dist

    # ======================================================================
    # RL/PettingZoo API
    # ======================================================================
    @property
    def observation_space(self):
        return [self.observation_spaces[a] for a in self.possible_agents]

    @property
    def action_space(self):
        return [self.action_spaces[a] for a in self.possible_agents]

    @property
    def share_observation_space(self):
        return [self.share_observation_spaces for _ in self.possible_agents]

    def seed(self, seed):
        random.seed(seed)
        np.random.seed(seed)

    # ----------------------------------------------------------------
    # [2] ê´€ì¸¡ (Observation) - ê±°ë¦¬ìˆœ ì •ë ¬ + Padding
    # ----------------------------------------------------------------
    def _get_obs(self, agent):
        # 1. ë‚´ ìƒíƒœ (IMU ì†ë„) - ì´ê±´ ë‚´ ì •ë³´ë‹ˆê¹Œ GT ì¨ë„ ë¨ (í˜¹ì€ getImuData ì‚¬ìš©)
        try:
            my_state = self.client.getMultirotorState(vehicle_name=agent)
            vx = my_state.kinematics_estimated.linear_velocity.x_val
            vy = my_state.kinematics_estimated.linear_velocity.y_val
            norm_vx = np.clip(vx / self.MAX_SPEED, -1.0, 1.0)
            norm_vy = np.clip(vy / self.MAX_SPEED, -1.0, 1.0)
        except:
            norm_vx, norm_vy = 0.0, 0.0

        # 2. ë¦¬ë” (ê³ ì • 1ê°œ)
        lx, ly, _ = self._get_lidar_measurement(agent, "Drone1")
        # ì •ê·œí™” (100m ê¸°ì¤€)
        _leader_feats = [np.clip(lx/100.0, -1, 1), np.clip(ly/100.0, -1, 1)]

        # 3. ì•„êµ° (KNN + Padding)
        other_agents = [a for a in self.possible_agents if a != agent]
        ally_candidates = []
        for other in other_agents:
            ox, oy, _ = self._get_lidar_measurement(agent, other)
            dist_sq = ox**2 + oy**2
            ally_candidates.append({"d": dist_sq, "v": [np.clip(ox/100.0, -1, 1), np.clip(oy/100.0, -1, 1)]})
        
        ally_candidates.sort(key=lambda x: x["d"])
        _ally_feats = []
        for i in range(self.K_ally):
            if i < len(ally_candidates): _ally_feats.extend(ally_candidates[i]["v"])
            else: _ally_feats.extend([0.0, 0.0]) # Padding

        # 4. ì êµ° (KNN + Padding + Pure Lidar Dynamics)
        enemy_candidates = []
        for e_name in self.enemy_names:
            # (1) Lidarë¡œ ìœ„ì¹˜ ì¸¡ì • (Meter ë‹¨ìœ„)
            ex, ey, detected = self._get_lidar_measurement(agent, e_name)
            
            # (2) ì´ì „ ìœ„ì¹˜ì™€ ë¹„êµí•˜ì—¬ Closing Speed, LOS Rate ê³„ì‚° (NO GT!)
            if detected:
                c_speed, l_rate = self._calculate_lidar_dynamics(agent, e_name, ex, ey)
            else:
                # ê°ì§€ ëª»í–ˆìœ¼ë©´ ì†ë„ ì •ë³´ ëª¨ë¦„ (0 ì²˜ë¦¬)
                c_speed, l_rate = 0.0, 0.0
            
            dist_sq = ex**2 + ey**2
            
            # Feature: [rel_x, rel_y, closing, los] (ëª¨ë‘ ì •ê·œí™”ë¨)
            feats = [
                np.clip(ex/100.0, -1, 1), 
                np.clip(ey/100.0, -1, 1), 
                c_speed, 
                l_rate
            ]
            enemy_candidates.append({"d": dist_sq, "v": feats})

        enemy_candidates.sort(key=lambda x: x["d"])
        _enemy_feats = []
        for i in range(self.K_enemy):
            if i < len(enemy_candidates): _enemy_feats.extend(enemy_candidates[i]["v"])
            else: _enemy_feats.extend([0.0, 0.0, 0.0, 0.0]) # Padding

        # 5. ìµœì¢… ê²°í•©
        obs = np.concatenate([
            _leader_feats,
            _ally_feats,
            _enemy_feats,
            [norm_vx, norm_vy]
        ], dtype=np.float32)
        
        return obs
    

    def _do_action(self, actions):
        actions = np.clip(actions, -1, 1)
        dt = self.dt

        for i, agent in enumerate(self.possible_agents):
            a = actions[i]
            # [2D UFO Mode]
            # a[0]: ì „í›„ (Global X), a[1]: ì¢Œìš° (Global Y)
            # YawRate: 0 (Fixed)
            
            v_forward = float(a[0]) * self.MAX_SPEED  
            v_lateral = float(a[1]) * self.MAX_SPEED

            sp = math.hypot(v_forward, v_lateral)
            if sp > self.MAX_SPEED:
                s = self.MAX_SPEED / (sp + 1e-6)
                v_forward *= s
                v_lateral *= s

            vx = v_forward 
            vy = v_lateral 

            yaw_mode = airsim.YawMode(is_rate=False, yaw_or_rate=0.0)

            self.client.moveByVelocityZAsync(
                vx=vx, vy=vy, z=self.fixed_z, duration=dt,
                yaw_mode=yaw_mode,
                vehicle_name=agent
            )

    def _get_rewards(self, per_agent_results):
        return [np.mean(per_agent_results) for _ in self.possible_agents]

    def reset(self, seed=None, options=None):
        self.episode_count += 1
        print(f"ì—í”¼ì†Œë“œ: {self.episode_count} | ì†Œë¹„í•œ ìŠ¤í… ìˆ˜: {self.step_count}")

        self.step_count = 0
        self.agents = self.possible_agents[:]

        self.client.reset()

        # [ì¶”ê°€] ì‹œë®¬ë ˆì´í„° ë‚´ì˜ ì êµ°(DynamicObstacleë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ê°ì²´) ìë™ ê°ì§€
        all_vehicles = self.client.listVehicles()
        self.enemy_names = sorted([v for v in all_vehicles if v.startswith("DynamicObstacle")])
        if not self.enemy_names:
            # ì ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¦„ í•˜ë‚˜ ì¶”ê°€ (íŒ¨ë”©ìœ¼ë¡œ ì²˜ë¦¬ë¨)
            self.enemy_names = ["DynamicObstacle"] 
        
        # ê¸°ë³¸ íƒ€ê²Ÿ ì„¤ì • (ë³´ìƒ ê³„ì‚°ìš©, ê°€ì¥ ì²« ë²ˆì§¸ ì ì„ ë©”ì¸ìœ¼ë¡œ ê°€ì •í•˜ê±°ë‚˜ ë¡œì§ ìˆ˜ì • ê°€ëŠ¥)
        self.dynamic_name = self.enemy_names[0]

        self._setup_flight()
        self.client.simFlushPersistentMarkers()

        self._reset_obstacle_logic()
        self._get_current_location()

        self._prev_d_agent_enemy = {a: None for a in self.possible_agents}
        self._last_pose.clear()
        self._last_time.clear()

        self._last_action = {a: np.zeros(3, dtype=np.float32) for a in self.possible_agents}
        self._current_action = {a: np.zeros(3, dtype=np.float32) for a in self.possible_agents}

        # ë©”ëª¨ë¦¬ ë° ì´ì „ ìœ„ì¹˜ ì´ˆê¸°í™”
        self.lidar_memory = {}
        self.prev_lidar_pos = {}
        
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        for agent in self.possible_agents:
            self.lidar_memory[agent] = {}
            self.prev_lidar_pos[agent] = {}
            # ë³¸ì¸ê³¼ ë¦¬ë”
            self.lidar_memory[agent]["Drone1"] = [1.0, 1.0]
            self.prev_lidar_pos[agent]["Drone1"] = [100.0, 100.0]
            
            # ëª¨ë“  ì ì¬ì  ì êµ° ì´ˆê¸°í™”
            for e_name in self.enemy_names:
                self.lidar_memory[agent][e_name] = [1.0, 1.0]
                self.prev_lidar_pos[agent][e_name] = [100.0, 100.0]

            # ì•„êµ°ë“¤
            for other in self.possible_agents:
                if agent != other:
                    self.lidar_memory[agent][other] = [1.0, 1.0]
                    self.prev_lidar_pos[agent][other] = [100.0, 100.0]

        obs_list = [self._get_obs(a) for a in self.agents]
        return obs_list

    def step(self, actions):
        self.step_count += 1

        if self.step_count >= 300:
            print(f"â³[ì‹œê°„ ì´ˆê³¼] ìŠ¤í… {self.step_count} ë„ë‹¬! â†’ ì‹¤íŒ¨ ì²˜ë¦¬")
            return self._end_episode(self.REWARD_LEADER_HIT, "FAIL_TIMEOUT_LEADER_HIT")
        
        per_agent_obs, per_agent_results, per_agent_infos = [], [], []

        self._do_action(actions)
        self._update_leader_movement()
        self._update_dynamic_obstacle()
        self._get_current_location() # ì¶©ëŒ íŒì •ìš©

        for agent in self.possible_agents:
            # ì´íƒˆ ì²´í¬
            _distance_leader = np.linalg.norm([
                self.current_location[agent].position.x_val - self.current_location["Drone1"].position.x_val,
                self.current_location[agent].position.y_val - self.current_location["Drone1"].position.y_val,
                self.current_location[agent].position.z_val - self.current_location["Drone1"].position.z_val
            ])

            if _distance_leader > self.far_cutoff:
                print(f"[ì´íƒˆ] {agent} ë¦¬ë” ê±°ë¦¬ ì´ˆê³¼! â†’ ì „ì²´ ì‹¤íŒ¨")
                return self._end_episode(self.REWARD_AGENT_CRASH, "FAIL_AGENT_FAR_CUTOFF")

            # ë¦¬ë”ì™€ ì¶©ëŒ
            hit, d = self._check_distance_collision(agent, "Drone1", threshold=self.COLLISION_THRESHOLD)
            if hit:
                print(f"âš ï¸ğŸ’”[ì¶©ëŒ] {agent} â†” Leader")
                return self._end_episode(self.REWARD_AGENT_CRASH, "FAIL_DISTANCE_AGENT_LEADER")

            # ì•„êµ°ë¼ë¦¬ ì¶©ëŒ
            other_agents = [a for a in self.possible_agents if a != agent]
            for other in other_agents:
                hit, d = self._check_distance_collision(agent, other, threshold=self.COLLISION_THRESHOLD)
                if hit:
                    print(f"ğŸ’¥ğŸ¤–[ì¶©ëŒ] {agent} â†” {other}")
                    return self._end_episode(self.REWARD_AGENT_CRASH, "FAIL_DISTANCE_AGENT_AGENT")

            # ë¦¬ë” í”¼ê²© (Enemy -> Leader)
            collisionInfo = self.client.simGetCollisionInfo("Drone1")
            if collisionInfo.has_collided and collisionInfo.object_name == self.dynamic_name:
                print(f"ğŸ’¥[í”¼ê²©] ë¦¬ë”ê°€ ì ì—ê²Œ í”¼ê²©ë¨!")
                return self._end_episode(self.REWARD_LEADER_HIT, "FAIL_LEADER_AND_DYNAMIC_OBSTACLE_COLLISION")

            # ìš”ê²© ì„±ê³µ (Agent -> Enemy)
            hit, d = self._check_distance_collision(agent, self.dynamic_name, threshold=self.COLLISION_THRESHOLD)
            if hit:
                print(f"ğŸ¯ğŸ”¥[ìš”ê²©] {agent}ê°€ ì ì„ ìš”ê²© ì„±ê³µ!")
                return self._end_episode(self.REWARD_HIT_ENEMY, "SUCCESS_DISTANCE_AGENT_DYNAMIC")

           # --- [ê´€ì¸¡ ë° ë³´ìƒ] ---
            
            # 1. ê´€ì¸¡ (ì—¬ê¸°ì„œ self.lidar_memoryê°€ ê°±ì‹ ë¨!)
            obs = self._get_obs(agent)
            per_agent_obs.append(obs)

            # 2. ë³´ìƒ (ê°±ì‹ ëœ ë©”ëª¨ë¦¬ì™€ ì´ì „ ìœ„ì¹˜ë¥¼ ë¹„êµí•˜ì—¬ ê³„ì‚°)
            _reward = self._compute_reward(agent)
            per_agent_results.append(_reward)
            per_agent_infos.append({"step_reward": _reward})
            
            # 3. "ëª¨ë“ " ì êµ°ì— ëŒ€í•´ ì´ì „ ìœ„ì¹˜ ê°±ì‹  (ì¤‘ìš”!)
            targets_to_update = self.enemy_names + ["Drone1"]
            
            for t_name in targets_to_update:
                if t_name in self.lidar_memory[agent]:
                    norm_curr = self.lidar_memory[agent][t_name]
                    # ì •ê·œí™” í’€ì–´ì„œ ì €ì¥ (ë¯¸í„° ë‹¨ìœ„)
                    real_curr = [norm_curr[0] * 100.0, norm_curr[1] * 100.0]
                    self.prev_lidar_pos[agent][t_name] = real_curr

        termination_list = [False for _ in self.possible_agents]
        rewards_list = self._get_rewards(per_agent_results)
        obs_list = per_agent_obs
        infos_list = per_agent_infos

        return obs_list, rewards_list, termination_list, infos_list