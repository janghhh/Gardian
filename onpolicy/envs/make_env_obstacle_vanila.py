import gym
import numpy as np
import airsim
import math
import time
import random
from collections import deque

# DBSCAN ë° í´ëŸ¬ìŠ¤í„°ë§ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œê±° (ì‚¬ìš© ì•ˆ í•¨)
# from sklearn.cluster import DBSCAN 
# from sklearn.neighbors import NearestNeighbors 

class AirSimMultiDroneEnv:
    metadata = {"render_modes": []}

    def __init__(
        self,
        ip_address="127.0.0.1",                 # ê¸°ë³¸ê°’
        follower_names=("Follower0", "Follower1", "Follower2"),
        lidar_name="LidarSensor",               
        min_samples=5, # <--- ë¯¸ì‚¬ìš©
        step_length=0.01,
        fixed_z=-10.0,
        leader_velocity=1.0,                    
        optimal_distance=10.0,                  
        far_cutoff=60.0,                        
        too_close=0.5,                          
        dt=3.0,                                
        do_visualize=True                       
    ):
        super().__init__()
        self.possible_agents = list(follower_names)
        self.agents = self.possible_agents[:]

        # ì¶©ëŒ ê´€ë ¨ ì„¤ì •
        self.COLLISION_THRESHOLD = 1.0 # ëª¨ë“  ê±°ë¦¬ ê¸°ë°˜ ì¶©ëŒ íŒë‹¨ ì„ê³„ê°’ (m)
        self.STOP_DISTANCE_LEADER_OBSTACLE = 2.0 # ìœ ì¸ê¸°-ì¥ì• ë¬¼ ì¶©ëŒ ì„ê³„ê°’ (m)
        
        # ì†ë„/ì•¡ì…˜ ë²„í¼
        self.vmax_self = 3.0            
        self._timestep = 1.0

        # ì—ì´ì „íŠ¸/ë¦¬ë” ì†ë„ ì‚°ì¶œìš© ë²„í¼
        self._last_pose = {}
        self._last_time = {}

        # ì•¡ì…˜ ë²„í¼
        self._last_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self._current_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self.success_steps = {a: 0 for a in self.possible_agents}

        self.use_teleport = False  

        # í´ë¼ì´ì–¸íŠ¸
        self.client = airsim.MultirotorClient(ip=ip_address)
        self.client.confirmConnection()

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°/í™˜ê²½ íŒŒë¼ë¯¸í„°
        self.step_length = float(step_length)
        self.fixed_z = float(fixed_z)
        self.dt = float(dt)
        self.do_visualize = bool(do_visualize)
        self.max_cmd_speed = self.step_length / self.dt
        self.leader_velocity = float(leader_velocity)
        self.optimal_distance = float(optimal_distance)
        self.far_cutoff = float(far_cutoff)
        self.too_close = float(too_close)
        self.follower_names = list(follower_names)

        self._first_setup = True
        self.leader_stop = False

        # ===== obs / act / share_obs spaces =====
        self.K_ally = len(follower_names) - 1 # ë‚˜ë¥¼ ì œì™¸í•œ ì•„êµ° ìˆ˜
        self.K_enemy = 1                     # ë™ì  ì¥ì• ë¬¼ 1ëŒ€
        self.num_ally = self.K_ally
        self.num_enemy = self.K_enemy
        
        obs_dim = 3 + 2 + 2 * self.num_ally + 2 * self.num_enemy
        share_obs_dim = obs_dim * len(self.possible_agents)

        low_self = [-20.0, -20.0, -20.0]
        high_self = [20.0, 20.0, 20.0]
        low_bearing = -1.0
        high_bearing = 1.0
        low_dist = 0.0
        high_dist = 200.0

        per_agent_low = low_self + [low_bearing, low_dist] + [low_bearing, low_dist] * self.num_ally + [low_bearing, low_dist] * self.num_enemy
        per_agent_high = high_self + [high_bearing, high_dist] + [high_bearing, high_dist] * self.num_ally + [high_bearing, high_dist] * self.num_enemy

        self.observation_spaces = {
            agent: gym.spaces.Box(
                low=np.array(per_agent_low, dtype=np.float32),
                high=np.array(per_agent_high, dtype=np.float32),
                shape=(obs_dim,), dtype=np.float32
            ) for agent in self.possible_agents
        }

        self.num_yaw_bins = 9
        self.forward_speed = 3
        self.action_spaces = {
            agent: gym.spaces.Discrete(self.num_yaw_bins)
            for agent in self.possible_agents
        }

        self.share_observation_spaces = gym.spaces.Box(
            low=np.array(per_agent_low * len(self.possible_agents), dtype=np.float32),
            high=np.array(per_agent_high * len(self.possible_agents), dtype=np.float32),
            shape=(share_obs_dim,), dtype=np.float32
        )

        self.current_waypoint_idx = 0
        self.dynamic_name = "DynamicObstacle"
        self._setup_flight()
        self._generate_leader_waypoints()
        self._last_visualize_t = time.time()

    # ======================================================================
    # í—¬í¼ ë©”ì„œë“œ: í¬ì¦ˆ/ì†ë„/ê´€ì¸¡ ê´€ë ¨
    # ======================================================================
    def _quat_to_rot(self, q):
        w, x, y, z = q.w_val, q.x_val, q.y_val, q.z_val
        ww, xx, yy, zz = w*w, x*x, y*y, z*z
        R = np.array([
            [ww + xx - yy - zz, 2*(x*y - w*z), 2*(x*z + w*y)],
            [2*(x*y + w*z), ww - xx + yy - zz, 2*(y*z - w*x)],
            [2*(x*z - w*y), 2*(y*z + w*x), ww - xx - yy + zz]
        ], dtype=np.float32)
        return R

    def _get_pose_xyz(self, name):
        # simGetObjectPoseëŠ” 'object_name'ì„ ì¸ìˆ˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        pose = self.client.simGetObjectPose(object_name=name).position
        return np.array([pose.x_val, pose.y_val, pose.z_val], dtype=np.float32)

    def _get_yaw(self, name: str) -> float:
        pose = self.client.simGetVehiclePose(vehicle_name=name)
        _, _, yaw = airsim.to_eularian_angles(pose.orientation)
        return float(yaw)

    def _bearing_and_distance(self, src_pos, src_yaw, tgt_pos):
        dx = float(tgt_pos[0] - src_pos[0])
        dy = float(tgt_pos[1] - src_pos[1])
        dist = math.hypot(dx, dy)
        angle_world = math.atan2(dy, dx)
        bearing = angle_world - src_yaw
        while bearing > math.pi:
            bearing -= 2.0 * math.pi
        while bearing < -math.pi:
            bearing += 2.0 * math.pi
        bearing_norm = bearing / math.pi  # [-1, 1]
        return bearing_norm, dist
    
    def _targets_to_polar_feats(self, agent: str, targets: list, K: int, yaw: float, pos: np.ndarray):
        feats = []
        for i in range(K):
            if i < len(targets):
                target = targets[i]
                if isinstance(target, tuple) and len(target) == 2:
                    cpos = target[1]
                else:
                    cpos = target

                bearing, dist = self._bearing_and_distance(pos, yaw, cpos)
                feats.extend([bearing, dist])
            else:
                feats.extend([0.0, 0.0])  # íŒ¨ë”©
        return np.array(feats, dtype=np.float32)

    def _get_self_velocity(self, name, now_t, current_pos):
        pos = current_pos
        v = np.zeros(3, np.float32)
        if name in self._last_pose:
            dt = max(1e-6, now_t - self._last_time.get(name, now_t))
            v = (pos - self._last_pose[name]) / dt
        self._last_pose[name] = pos
        self._last_time[name] = now_t

        v = np.clip(v, -self.vmax_self, self.vmax_self)
        prev = getattr(self, "_vel_ema_" + name, v)
        v_ema = 0.7 * prev + 0.3 * v
        setattr(self, "_vel_ema_" + name, v_ema)
        return v_ema
    
    # ======================================================================
    # ì´ˆê¸°í™”/ì´ë™/ì‹œê°í™” ê´€ë ¨
    # ======================================================================
    def _setup_flight(self):
        if self._first_setup:
            self.client.reset()
            self.client.enableApiControl(True, vehicle_name="Drone1")
            self.client.armDisarm(True, vehicle_name="Drone1")
            for agent in self.possible_agents:
                self.client.enableApiControl(True, vehicle_name=agent)
                self.client.armDisarm(True, vehicle_name=agent)

            self.client.enableApiControl(True, vehicle_name=self.dynamic_name)
            self.client.armDisarm(True, vehicle_name=self.dynamic_name)

            futs = [self.client.takeoffAsync(vehicle_name="Drone1")]
            futs += [self.client.takeoffAsync(vehicle_name=a) for a in self.possible_agents]
            futs += [self.client.takeoffAsync(vehicle_name=self.dynamic_name)]
            for f in futs:
                f.join()

            time.sleep(1.0)
            self._teleport_to_start()
            self._first_setup = False

    def _teleport_to_start(self):
        leader_start_x, leader_start_y = 5.0, 2.5
        radius = random.uniform(80.0, 90.0)
        angle = random.uniform(0, 2 * np.pi)
        obstacle_start_x = leader_start_x + radius * np.cos(angle)
        obstacle_start_y = leader_start_y + radius * np.sin(angle)

        start_cfg = {
            "Drone1": (5.0, 2.5, self.fixed_z),
            "Follower0": (0.0, 0.0, self.fixed_z),
            "Follower1": (0.0, 2.5, self.fixed_z),
            "Follower2": (0.0, 5.0, self.fixed_z),
            self.dynamic_name: (obstacle_start_x, obstacle_start_y, self.fixed_z),
        }

        self.client.enableApiControl(True, vehicle_name="Drone1")
        for agent in self.possible_agents:
            self.client.enableApiControl(True, vehicle_name=agent)

        for name, (x, y, z) in start_cfg.items():
            px, py, pz = float(x), float(y), float(z)
            self.client.simSetVehiclePose(
                airsim.Pose(
                    airsim.Vector3r(px, py, pz),
                    airsim.Quaternionr(0.0, 0.0, 0.0, 1.0)
                ),
                ignore_collision=True,
                vehicle_name=name
            )
        time.sleep(0.05)

    def _generate_leader_waypoints(self):
        leader_start_pos = np.array([5.0, 2.5, self.fixed_z])
        distance = random.uniform(110.0, 130.0)
        angle = random.uniform(0, 2 * np.pi)
        final_destination = leader_start_pos + np.array([
            distance * np.cos(angle),
            distance * np.sin(angle),
            0.0
        ])
        self.leader_waypoints = [final_destination]
        self.current_waypoint_idx = 0
        
        object_name = "target1v1_5"
        try:
            flag_position = airsim.Vector3r(
                float(final_destination[0]),
                float(final_destination[1]),
                float(self.fixed_z + 8.0)
            )
            flag_orientation = airsim.to_quaternion(0, 80.1, 0)
            flag_pose = airsim.Pose(flag_position, flag_orientation)
            self.client.simSetObjectPose(object_name, flag_pose)
        except Exception as e:
            print(f"'{object_name}' ê°ì²´ ì´ë™ ì‹¤íŒ¨(ì–¸ë¦¬ì–¼ì— ì—†ì„ ìˆ˜ ìˆìŒ): {e}")

    def _update_leader_movement(self):
        """
        ìœ ì¸ê¸°ë¥¼ 'Zì¶• ìœ„ì¹˜ ê³ ì •' ëª…ë ¹ìœ¼ë¡œ í˜¸ë²„ë§í•˜ë„ë¡ ìˆ˜ì •
        """
        
        # 1. Zì¶• 'ìœ„ì¹˜'ë¥¼ ì§ì ‘ ì œì–´í•˜ëŠ” moveByVelocityZAsync ì‚¬ìš©
        self.client.moveByVelocityZAsync(
            0.0, 0.0,
            float(self.fixed_z), # 0ì´ ì•„ë‹Œ, ëª©í‘œ ê³ ë„(ì˜ˆ: -20.0)ë¥¼ ì§€ì •
            duration=self.dt, 
            vehicle_name="Drone1"
        )

        # 2. â˜… ìˆ˜ì •ëœ ë¶€ë¶„: ì‹œê°í™” ì£¼ê¸°ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ë¹„í™œì„±í™” (í•™ìŠµ ì†ë„ ê°œì„  ëª©ì )
        if self.do_visualize:
            now = time.time()
            # 0.5ì´ˆ ê°„ê²©ìœ¼ë¡œ ì‹œê°í™” ì£¼ê¸°ë¥¼ ëŠ˜ë ¤ GPU/CPU ë¶€í•˜ ê°ì†Œ
            if (now - self._last_visualize_t) >= 0.5: 
                self.client.simFlushPersistentMarkers()
                self._visualize_circles()
                self._last_visualize_t = now

        # 3. ìœ ì¸ê¸°ê°€ ì›€ì§ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ë¯¸ì…˜ ì„±ê³µ(True)ì€ ë°˜í™˜í•˜ì§€ ì•ŠìŒ


    def _visualize_circles(self):
        try:
            leader_pos = self.client.simGetObjectPose("Drone1").position
            center = np.array([leader_pos.x_val, leader_pos.y_val, leader_pos.z_val], dtype=float)

            def ring_points(radius, n=36):
                pts = []
                for i in range(n + 1):
                    ang = (i / n) * 2 * np.pi
                    x = center[0] + radius * np.cos(ang)
                    y = center[1] + radius * np.sin(ang)
                    z = center[2]
                    pts.append(airsim.Vector3r(x, y, z))
                return pts

            line_thickness = 20.0
            self.client.simPlotLineStrip(
                ring_points(self.optimal_distance),
                [1, 1, 0, 0.8],
                line_thickness, 0.15, True
            )
            self.client.simPlotLineStrip(
                ring_points(self.far_cutoff),
                [0, 1, 0, 0.8],
                line_thickness, 0.15, True
            )
        except Exception:
            pass
        

    # ======================================================================
    # ë³´ìƒ/ì¢…ë£Œ ê´€ë ¨
    # ======================================================================
    def _formation_reward(self, agent_pos, leader_pos):
        rel = leader_pos - agent_pos
        dist = math.hypot(float(rel[0]), float(rel[1]))
        if dist < 0.5 or dist > 60.0:
            return -5.0
        ideal = 10.0
        sigma = 10.0
        r = 1.5 * math.exp(-((dist - ideal) ** 2) / (2.0 * sigma ** 2)) - 0.5
        return r

    def _guardian_reward(self, agent_pos, leader_pos, dynamic_pos):
        d_lo = np.linalg.norm(leader_pos[:2] - dynamic_pos[:2])
        d_ao = np.linalg.norm(agent_pos[:2] - dynamic_pos[:2])

        ALERT_DIST = 80.0
        if d_lo > ALERT_DIST:
            return 0.0

        if d_ao < d_lo:
            score = (d_lo - d_ao) / max(d_lo, 1e-3)
            return 5.0 * score
        else:
            return -2.0

    def _compute_reward(self, agent, all_poses, dynamic_pos):
        agent_pos = all_poses[agent]
        leader_pos = all_poses["Drone1"]
        
        # 1) ìœ ì¸ê¸°ì— ë„ˆë¬´ ê°€ê¹Œì›€ â†’ í° íŒ¨ë„í‹° + ì¢…ë£Œ (ê±°ë¦¬ ê¸°ë°˜)
        if np.linalg.norm(agent_pos[:2] - leader_pos[:2]) < 1.0:
            return -200.0, True 

        # 3) í¬ë©”ì´ì…˜ ë³´ìƒ
        r_form = self._formation_reward(agent_pos, leader_pos)

        # 4) ê°€ë””ì–¸ ìœ„ì¹˜ ë³´ìƒ
        r_guard = self._guardian_reward(agent_pos, leader_pos, dynamic_pos)

        # 5) ë¦¬ë”ê°€ ì´ë¯¸ ì¥ì• ë¬¼ì— ë§ì•„ ë©ˆì¶˜ ìƒíƒœë¼ë©´ íŒ¨ë„í‹° ì£¼ê³  ì¢…ë£Œ
        if self.leader_stop:
            return -150.0, True

        r_total = r_form + r_guard
        return float(r_total), False

    def _team_reward_and_done(
        self,
        per_agent_results,
        intercepted_agent=None,
        hit_leader=False,
        failed_status=None  # <-- í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ì— ì¶”ê°€
    ):
        
        if failed_status is not None:
            # ì—ì´ì „íŠ¸ ì¶©ëŒë¡œ ì¸í•œ ì¦‰ì‹œ ì¢…ë£Œ (stepì—ì„œ í˜¸ì¶œë¨)
            return -800.0, True, {"final_status": failed_status}

        if hit_leader:
            # ì¥ì• ë¬¼ì´ ìœ ì¸ê¸°ì— ë‹¿ìŒ â†’ ìµœì•…ì˜ ì‹¤íŒ¨ (ì¢…ë£Œ ì¡°ê±´ 2)
            return -800.0, True, {"final_status": "FAIL_HIT_LEADER"}

        if intercepted_agent is not None:
            # ì¥ì• ë¬¼ì´ íŒ”ë¡œì›Œ ë“œë¡ ì— ë¨¼ì € ë¶€ë”ªí˜ (ìš”ê²©) -> ë³´ìƒë§Œ ì£¼ê³  ê³„ì† ì§„í–‰
            tracking_rewards = [r_i for (r_i, _) in per_agent_results]
            base = float(np.mean(tracking_rewards))
            return base + 1000.0, False, { 
                "final_status": "CONTINUE_INTERCEPT",
                "interceptor": intercepted_agent,
            }

        tracking_rewards = [r_i for (r_i, _) in per_agent_results]
        return float(np.mean(tracking_rewards)), False, {}
    
    def _end_episode(self, reward, status):
        """
        ì—í”¼ì†Œë“œ ì¢…ë£Œ í—¬í¼ (ì¶©ëŒ ì´ë²¤íŠ¸ ë°œìƒ ì‹œ)
        """
        n = len(self.agents)
        rewards_list = [reward] * n
        dones_list = [True] * n
        
        all_poses = {}
        all_poses["Drone1"] = self._get_pose_xyz("Drone1")
        all_poses[self.dynamic_name] = self._get_pose_xyz(self.dynamic_name)
        for agent in self.agents:
            all_poses[agent] = self._get_pose_xyz(agent)

        now_t = self._timestep * self.dt
        obs_list = [self._get_obs(a, now_t, all_poses) for a in self.agents]
        
        infos_list = [{"final_status": status, "reward": reward}] * n
        
        return obs_list, rewards_list, dones_list, infos_list


    # --------------------- ë™ì ì¥ì• ë¬¼ FSM ---------------------
    def _update_dynamic_obstacle(self, t):
        name = self.dynamic_name
        fixed_z = self.fixed_z
        attack_speed = 2.5
        STOP_DISTANCE = self.STOP_DISTANCE_LEADER_OBSTACLE 

        # ìƒíƒœ ì´ˆê¸°í™”
        if not hasattr(self, "_obstacle_state"):
            self._obstacle_state = "IDLE"
            self._next_chase_time = time.time() + random.uniform(0.0, 1.0)
            self._idle_pos = None
            self._chase_mode = None

        # í˜„ì¬ ìœ„ì¹˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë¶ˆí•„ìš”í•œ ë¦¬ë” ìœ„ì¹˜ ì¡°íšŒ ì œê±° ê°€ëŠ¥í•˜ì§€ë§Œ, ê±°ë¦¬ ì²´í¬ ìœ„í•´ ìœ ì§€)
        leader_pose = self.client.simGetObjectPose("Drone1").position
        obstacle_pose = self.client.simGetObjectPose(name).position
        lx, ly, lz = leader_pose.x_val, leader_pose.y_val, leader_pose.z_val
        cx, cy, cz = obstacle_pose.x_val, obstacle_pose.y_val, obstacle_pose.z_val

        dx, dy = lx - cx, ly - cy
        dist_2d = math.sqrt(dx ** 2 + dy ** 2) + 1e-9

        # ---------------------------------------------------------
        # 1. IDLE ìƒíƒœ
        # ---------------------------------------------------------
        if self._obstacle_state == "IDLE":
            if self._idle_pos is None:
                radius = random.uniform(90.0, 100.0)
                angle = random.uniform(0, 2 * np.pi)
                self._idle_pos = (lx + radius * np.cos(angle), ly + radius * np.sin(angle))

            ix, iy = self._idle_pos
            dx_i, dy_i = ix - cx, iy - cy
            dist_idle = math.sqrt(dx_i ** 2 + dy_i ** 2)
            
            if dist_idle > 1.0:
                vx = dx_i / dist_idle * 2.0
                vy = dy_i / dist_idle * 2.0
                vz = (fixed_z - cz) * 3.0
                self.client.moveByVelocityAsync(float(vx), float(vy), float(vz), duration=1.0, vehicle_name=name)
            else:
                self.client.moveByVelocityAsync(0.0, 0.0, 0.0, duration=1.0, vehicle_name=name)

            # ì¶”ê²© ì‹œê°„ ë„ë‹¬ ì‹œ -> ìƒíƒœ ë³€ê²½ ë° â˜…ë°©í–¥ ê³ ì •â˜…
            if time.time() > self._next_chase_time:
                self._obstacle_state = "CHASE"
                self._chase_start = time.time()
                
                # â˜…â˜…â˜… [í•µì‹¬ ìˆ˜ì •] CHASE ì§„ì… ì‹œì ì— ë°©í–¥ ë²¡í„°ë¥¼ ë”± í•œ ë²ˆë§Œ ê³„ì‚°í•˜ê³  ì €ì¥ â˜…â˜…â˜…
                # ë¦¬ë”ë¥¼ í–¥í•œ ë‹¨ìœ„ ë²¡í„° ê³„ì‚°
                dir_x = dx / dist_2d
                dir_y = dy / dist_2d
                
                # ì†ë„ ë²¡í„° í™•ì • ë° ì €ì¥
                self._chase_vx = dir_x * attack_speed
                self._chase_vy = dir_y * attack_speed
                
                print(f"[ì¶”ê²© ì‹œì‘] ê³ ì • ì†ë„ ë²¡í„°: ({self._chase_vx:.2f}, {self._chase_vy:.2f})")
                return

        # ---------------------------------------------------------
        # 2. CHASE ìƒíƒœ (ê³„ì‚° ì—†ì´ ì €ì¥ëœ ì†ë„ë¡œ ì´ë™)
        # ---------------------------------------------------------
        elif self._obstacle_state == "CHASE":
            if dist_2d <= STOP_DISTANCE:
                print(f"ğŸ’¥[ë¦¬ë” í”¼ê²©] ì¶©ëŒ ì„ê³„ì  ë„ë‹¬! ì—í”¼ì†Œë“œ ì¢…ë£Œ.")
                self.client.moveByVelocityAsync(0.0, 0.0, 0.0, duration=1.0, vehicle_name=name)
                self.leader_stop = True
                return

            # â˜…â˜…â˜… ì¬ê³„ì‚° ë¡œì§ ì œê±°í•¨. ì €ì¥ëœ ê°’(_chase_vx, _chase_vy)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© â˜…â˜…â˜…
            # ê³ ë„ëŠ” ë†’ì´ë¥¼ ë§ì¶°ì•¼ í•˜ë¯€ë¡œ ê³„ì† ê³„ì‚° (ë‹¨ìˆœ ì—°ì‚°ì´ë¼ ë¶€í•˜ ì ìŒ)
            vz = (fixed_z - cz) * 8.0
            
            # ë§¤ ìŠ¤í… ë™ì¼í•œ ì†ë„ ëª…ë ¹ ì „ì†¡ (durationì„ ê¸¸ê²Œ ì¤¬ìœ¼ë¯€ë¡œ ë¶€ë“œëŸ½ê²Œ ì´ì–´ì§)
            self.client.moveByVelocityAsync(
                float(self._chase_vx), 
                float(self._chase_vy), 
                float(vz), 
                duration=1.0, 
                vehicle_name=name
            )

        # ---------------------------------------------------------
        # 3. RETURN ìƒíƒœ
        # ---------------------------------------------------------
        elif self._obstacle_state == "RETURN":
            if self._idle_pos is None:
                radius = random.uniform(90.0, 100.0)
                angle = random.uniform(0, 2 * np.pi)
                self._idle_pos = (lx + radius * math.cos(angle), ly + radius * math.sin(angle))

            ix, iy = self._idle_pos
            dx_r, dy_r = ix - cx, iy - cy
            dist_return = math.sqrt(dx_r ** 2 + dy_r ** 2)
            
            if dist_return > 1.0:
                vx = dx_r / dist_return * 4.0
                vy = dy_r / dist_return * 4.0
                vz = (fixed_z - cz) * 8.0
                self.client.moveByVelocityAsync(float(vx), float(vy), float(vz), duration=1.0, vehicle_name=name)
            else:
                self._obstacle_state = "IDLE"
                self._next_chase_time = time.time() + random.uniform(1.0, 3.0)

    def _teleport_obstacle_around_leader(self):

        # 1. ë¦¬ë”ì˜ í˜„ì¬ ìœ„ì¹˜ë¥¼ ì¡°íšŒí•˜ì§€ ì•Šê³ , ì—í”¼ì†Œë“œ ì‹œì‘ ê³ ì • ì¢Œí‘œ ì‚¬ìš©
        leader_start_x, leader_start_y = 5.0, 2.5 
        
        # 2. ì‹œì‘ ì‹œì ê³¼ ë™ì¼í•œ ë°˜ê²½ ë° ê°ë„ ì„¤ì •
        radius = random.uniform(80.0, 90.0)
        angle  = random.uniform(0, 2 * np.pi)
        
        # 3. ì¢Œí‘œ ê³„ì‚°
        ox = leader_start_x + radius * math.cos(angle)
        oy = leader_start_y + radius * math.sin(angle)
        oz = self.fixed_z

        # 4. ìœ„ì¹˜ ì„¤ì • (float í˜•ë³€í™˜ ìœ ì§€)
        self.client.simSetVehiclePose(
            airsim.Pose(
                airsim.Vector3r(float(ox), float(oy), float(oz)), 
                airsim.Quaternionr(0, 0, 0, 1)
            ),
            ignore_collision=True,
            vehicle_name=self.dynamic_name
        )
        
        # 5. ìˆœê°„ì´ë™ í›„ ì†ë„ 0ìœ¼ë¡œ ì´ˆê¸°í™” ë° ì •ì§€ (ìŠ¤í… ì‹œê°„ dt ë™ì•ˆ ìœ ì§€)
        self.client.moveByVelocityAsync(0.0, 0.0, 0.0, duration=self.dt, vehicle_name=self.dynamic_name)
        
    # ======================================================================
    # RL/PettingZoo API
    # ======================================================================
    @property
    def observation_space(self):
        return [self.observation_spaces[a] for a in self.possible_agents]

    @property
    def action_space(self):
        return [self.action_spaces[a] for a in self.possible_agents]

    @property
    def share_observation_space(self):
        return [self.share_observation_spaces for _ in self.possible_agents]

    def seed(self, seed):
        random.seed(seed)
        np.random.seed(seed)

    def reset(self, seed=None, options=None):
        self.agents = self.possible_agents[:]
        self._setup_flight()
        self._generate_leader_waypoints()
        self.current_waypoint_idx = 0
        self._teleport_to_start()
        self.client.enableApiControl(True, vehicle_name=self.dynamic_name)
        self.client.simFlushPersistentMarkers()

        self._timestep = 0
        self._last_pose.clear()
        self._last_time.clear()
        self._last_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self._current_action = {a: np.zeros(2, dtype=np.float32) for a in self.possible_agents}
        self.success_steps = {a: 0 for a in self.possible_agents}
        self.leader_stop = False # í”¼ê²© í”Œë˜ê·¸ ì´ˆê¸°í™”

        all_poses = {}
        all_poses["Drone1"] = self._get_pose_xyz("Drone1")
        all_poses[self.dynamic_name] = self._get_pose_xyz(self.dynamic_name)
        for agent in self.agents:
            all_poses[agent] = self._get_pose_xyz(agent)

        now_t = 0.0
        obs_list = [self._get_obs(a, now_t, all_poses) for a in self.agents]

        self._obstacle_state = "IDLE"
        self._idle_pos = None
        self._next_chase_time = time.time() + random.uniform(1.0, 3.0)
        self._chase_mode = None
        
        return obs_list

    def _get_obs(self, agent, now_t, all_poses):
        now_t = self._timestep * self.dt

        agent_pos = all_poses[agent]
        leader_pos = all_poses["Drone1"]
        dynamic_pos = all_poses[self.dynamic_name] # ë™ì  ì¥ì• ë¬¼ (ì êµ°) ìœ„ì¹˜

        self_vel = self._get_self_velocity(agent, now_t, agent_pos)
        yaw = self._get_yaw(agent)

        bearing_leader, dist_leader = self._bearing_and_distance(agent_pos, yaw, leader_pos)

        # 1. ì•„êµ° í”¼ì²˜ (ë‚˜ë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ íŒ”ë¡œì›Œë“¤)
        ally_positions = [all_poses[other_agent] for other_agent in self.agents if other_agent != agent]
        ally_feats = self._targets_to_polar_feats(agent, ally_positions, self.num_ally, yaw, agent_pos)

        # 2. ì êµ° í”¼ì²˜ (ë™ì  ì¥ì• ë¬¼)
        enemy_positions = [dynamic_pos]
        enemy_feats = self._targets_to_polar_feats(agent, enemy_positions, self.num_enemy, yaw, agent_pos)

        obs = np.concatenate(
            [
                self_vel.astype(np.float32),
                np.array([bearing_leader, dist_leader], dtype=np.float32),
                ally_feats,
                enemy_feats,
            ],
            axis=0
        ).astype(np.float32)

        return obs

    def _do_action(self, agent, action):
        if isinstance(action, (np.ndarray, list, tuple)):
            a_idx = int(action[0])
        else:
            a_idx = int(action)
        a_idx = np.clip(a_idx, 0, self.num_yaw_bins - 1)

        ratio = a_idx / (self.num_yaw_bins - 1) if self.num_yaw_bins > 1 else 0.5
        yaw_norm = -1.0 + 2.0 * ratio  # [-1, 1]

        self._last_action[agent] = np.array([yaw_norm, 0.0], dtype=np.float32)
        self._current_action[agent] = self._last_action[agent].copy()

        max_yaw_rate_deg = 90.0  # deg/s
        yaw_rate = math.radians(max_yaw_rate_deg) * yaw_norm  # [rad/s]

        # ----------------------------------------------------------------------
        # â˜… ìˆ˜ì •ëœ ë¶€ë¶„: ë‹¨ì¼ API í˜¸ì¶œë¡œ í†µí•© (ì´ì „ì˜ moveByRollPitchYawrateZAsync.join() ì œê±°)
        # ----------------------------------------------------------------------
        
        # í˜„ì¬ Yaw ê°ë„ ì½ê¸°
        pose = self.client.simGetVehiclePose(vehicle_name=agent)
        _, _, current_yaw = airsim.to_eularian_angles(pose.orientation)

        speed = self.forward_speed  # [m/s]
        
        # ëª©í‘œ YawëŠ” í˜„ì¬ Yawì—ì„œ íšŒì „ìœ¨ì„ ì ìš©í•˜ì—¬ dt ì‹œê°„ í›„ì˜ ê°ë„ë¥¼ ì¶”ì •
        # moveByVelocityZAsyncì— Yaw Rate ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒì „ê³¼ ì „ì§„ì„ ë™ì‹œì— ì§€ì‹œ
        
        # í˜„ì¬ Yaw ê°ë„ì™€ ì†ë„ë¥¼ ì´ìš©í•´ VX, VY ê³„ì‚° (Forward Speed)
        vx = speed * math.cos(current_yaw)
        vy = speed * math.sin(current_yaw)

        # moveByVelocityZAsyncë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ë„ì™€ Yaw Rateë¥¼ ë™ì‹œì— ì§€ì •
        # **duration=self.dt**ë¡œ ì„¤ì •í•˜ì—¬ ì „ì²´ ìŠ¤í… ì‹œê°„ ë™ì•ˆ ëª…ë ¹ì„ ìœ ì§€
        self.client.moveByVelocityZAsync(
            vx=vx, vy=vy, z=float(self.fixed_z),
            duration=self.dt, 
            drivetrain=airsim.DrivetrainType.MaxDegreeOfFreedom,
            # is_rate=Trueë¡œ Yaw Rateë¥¼ ì ìš©í•˜ì—¬ íšŒì „ì„ ì§€ì‹œ
            yaw_mode=airsim.YawMode(is_rate=True, yaw_or_rate=math.degrees(yaw_rate)),
            vehicle_name=agent
        )
        # ì´ ë¹„ë™ê¸° í˜¸ì¶œ í›„ .join()ì„ í˜¸ì¶œí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ë³‘ë ¬ ì—ì´ì „íŠ¸ ì²˜ë¦¬ê°€ ë¹¨ë¼ì§‘ë‹ˆë‹¤.
    
    def step(self, actions):
        self._timestep += 1
        t = time.time()
        
        # 1) ì—ì´ì „íŠ¸ ì•¡ì…˜ ì ìš©
        for agent, act in zip(self.agents, actions):
            self._do_action(agent, act)

        # 2) ìœ ì¸ê¸°/ì¥ì• ë¬¼ ì´ë™
        self._update_leader_movement()
        self._update_dynamic_obstacle(t) 

        # 3) ìœ„ì¹˜ë“¤ ìˆ˜ì§‘ (ì´ë™ í›„ ìœ„ì¹˜)
        all_poses = {}
        all_poses["Drone1"] = self._get_pose_xyz("Drone1")
        all_poses[self.dynamic_name] = self._get_pose_xyz(self.dynamic_name)
        for agent in self.agents:
            all_poses[agent] = self._get_pose_xyz(agent)
        dynamic_pos = all_poses[self.dynamic_name]
        leader_pos = all_poses["Drone1"]
        

        # ğŸ”¥ (0) ì´íƒˆ ì²´í¬: far_cutoff (ì´ˆë¡ì„ ) ì´ˆê³¼ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
        for agent in self.agents:
            agent_pos = all_poses[agent]
            # 2D ê±°ë¦¬ ê³„ì‚°
            dist_to_leader = np.linalg.norm(agent_pos[:2] - leader_pos[:2])
            
            if dist_to_leader > self.far_cutoff:
                print(f"âŒ[ì´íƒˆ] {agent}ì´ ë¦¬ë”ì™€ì˜ ê±°ë¦¬({dist_to_leader:.2f}m)ê°€ ì´íƒˆ ì„ê³„ê°’({self.far_cutoff}m) ì´ˆê³¼! â†’ ì „ì²´ ì‹¤íŒ¨")
                print(f"[í˜„ì¬ ìŠ¤í…: {self._timestep}]")
                # -1000.0ì˜ í° íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬í•˜ê³  ì—í”¼ì†Œë“œ ì¢…ë£Œ
                return self._end_episode(-1000.0, "FAIL_AGENT_FAR_CUTOFF")

        # ================================
        # ğŸ”¥ (0) ê±°ë¦¬ ê¸°ë°˜ ì¶©ëŒ/ì¢…ë£Œ ì²´í¬ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        # ================================
        intercepted_agent = None
        
        for i in range(len(self.agents)):
            agent_i = self.agents[i]
            pos_i = all_poses[agent_i]
            
            # A) Agent-Agent Collision (ê±°ë¦¬ ê¸°ë°˜ - ì¢…ë£Œ)
            for j in range(i + 1, len(self.agents)):
                agent_j = self.agents[j]
                pos_j = all_poses[agent_j]
                dist_aa = np.linalg.norm(pos_i[:2] - pos_j[:2])
                
                if dist_aa < self.COLLISION_THRESHOLD:
                    print(f"ğŸ’¥[ê±°ë¦¬ ì¶©ëŒ] {agent_i} â†” {agent_j} ({dist_aa:.2f}m) â†’ ì „ì²´ ì‹¤íŒ¨")
                    print(f"[í˜„ì¬ ìŠ¤í…: {self._timestep}]")
                    return self._end_episode(-1000.0, "FAIL_AGENT_COLLISION")

            # B) Agent-Leader Collision (ê±°ë¦¬ ê¸°ë°˜ - ì¢…ë£Œ)
            dist_al = np.linalg.norm(pos_i[:2] - leader_pos[:2])
            if dist_al < self.COLLISION_THRESHOLD:
                print(f"ğŸ’¥[ê±°ë¦¬ ì¶©ëŒ] {agent_i}ì´ Drone1ì— ë¶€ë”ªí˜ ({dist_al:.2f}m) â†’ ì „ì²´ ì‹¤íŒ¨")
                print(f"[í˜„ì¬ ìŠ¤í…: {self._timestep}]")
                return self._end_episode(-1000.0, "FAIL_AGENT_HIT_LEADER")
                
            # C) Agent-Obstacle Collision (ê±°ë¦¬ ê¸°ë°˜ ìš”ê²© - ê³„ì† ì§„í–‰)
            dist_ao = np.linalg.norm(pos_i[:2] - dynamic_pos[:2])
            if dist_ao < self.COLLISION_THRESHOLD and intercepted_agent is None:
                # ** ìš”ê²© ì„±ê³µ (ê±°ë¦¬): ë³´ìƒ í›„ ì¥ì• ë¬¼ ìˆœê°„ì´ë™ **
                print(f"ğŸ›¡ï¸[ê±°ë¦¬ ê°€ë””ì–¸] {agent_i}ì´ ë™ì ì¥ì• ë¬¼({self.dynamic_name})ì„ ìš”ê²©! ({dist_ao:.2f}m)")
                print(f"[í˜„ì¬ ìŠ¤í…: {self._timestep}]")
                self._teleport_obstacle_around_leader()
                intercepted_agent = agent_i
                
                # ì¥ì• ë¬¼ ìœ„ì¹˜ ì¦‰ì‹œ ê°±ì‹ 
                all_poses[self.dynamic_name] = self._get_pose_xyz(self.dynamic_name)
                dynamic_pos = all_poses[self.dynamic_name]
                
        # ================================
        # ğŸ”¥ (1) AirSim ì¶©ëŒ ì´ë²¤íŠ¸ ê²€ì‚¬ (ë¬¼ë¦¬ì  ì ‘ì´‰ í™•ì¸)
        # ================================
        for agent in self.agents:
            col = self.client.simGetCollisionInfo(vehicle_name=agent)
            if col.has_collided and col.object_name == self.dynamic_name:
                # Agent â†” Obstacle ë¬¼ë¦¬ ì¶©ëŒ (ìš”ê²©)
                if intercepted_agent is None: # ê±°ë¦¬ ì²´í¬ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                    print(f"ğŸ›¡ï¸[ì´ë²¤íŠ¸ ê°€ë””ì–¸] {agent}ì´ ë™ì ì¥ì• ë¬¼({self.dynamic_name})ì„ ë§‰ìŒ!")
                    print(f"[í˜„ì¬ ìŠ¤í…: {self._timestep}]")
                    self._teleport_obstacle_around_leader()
                    intercepted_agent = agent
        
        # ================================
        # ğŸ”¥ (2) Agent - Leader ë¬¼ë¦¬ ì¶©ëŒ ì´ë²¤íŠ¸ ê²€ì‚¬ (ìƒˆë¡œ ì¶”ê°€)
        # ================================
        for agent in self.agents:
            col = self.client.simGetCollisionInfo(vehicle_name=agent)
            
            # ì¶©ëŒì´ ë°œìƒí–ˆê³ , ê·¸ ëŒ€ìƒì´ ìœ ì¸ê¸°("Drone1")ì¸ ê²½ìš°
            if col.has_collided and col.object_name == "Drone1":
                print(f"ğŸ’¥[ì´ë²¤íŠ¸ ì¶©ëŒ] {agent}ì´ ìœ ì¸ê¸°(Drone1)ì™€ ë¬¼ë¦¬ ì¶©ëŒ ë°œìƒ!")
                print(f"[í˜„ì¬ ìŠ¤í…: {self._timestep}]")
                # ë¬¼ë¦¬ ì¶©ëŒ ë°œìƒ ì‹œ ê±°ë¦¬ ì¶©ëŒê³¼ ë™ì¼í•œ í° íŒ¨ë„í‹°ë¡œ ì¦‰ì‹œ ì—í”¼ì†Œë“œ ì¢…ë£Œ
                return self._end_episode(-1000.0, "FAIL_AGENT_PHYSICAL_HIT_LEADER")
                    
        # 4) ë¦¬ë” í”¼ê²© ì—¬ë¶€ ì²´í¬ (_update_dynamic_obstacleì—ì„œ ì„¤ì •ë¨)
        hit_leader = self.leader_stop

        # 5) ê´€ì¸¡ / ê°œë³„ ë³´ìƒ 
        obs_list, per_agent_results, per_agent_infos = [], [], []
        now_t = self._timestep * self.dt
        for agent in self.agents:
            o = self._get_obs(agent, now_t, all_poses)
            r, done_i = self._compute_reward(agent, all_poses, dynamic_pos) 

            obs_list.append(o)
            per_agent_results.append((float(r), bool(done_i)))
            per_agent_infos.append({"reward": float(r)})

        # 6) íŒ€ ë³´ìƒ / ì¢…ë£Œ íŒì •
        team_reward, done_all, final_team_info = self._team_reward_and_done(
            per_agent_results,
            intercepted_agent=intercepted_agent, 
            hit_leader=hit_leader,               
            failed_status=None, # ì¦‰ì‹œ ì¢…ë£ŒëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ None
        )

        n = len(self.agents)
        rewards_list = [team_reward] * n
        dones_list = [done_all] * n
        infos_list = []
        for i in range(n):
            info_i = per_agent_infos[i].copy()
            info_i.update(final_team_info)
            infos_list.append(info_i)

        return obs_list, rewards_list, dones_list, infos_list
    
    